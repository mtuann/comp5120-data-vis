{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv-sanity-lite.com/?page_number=1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "ROOT_URL = \"https://arxiv-sanity-lite.com/?page_number=%d\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crawl_arxiv_sanity_lite():\n",
    "    all_pages = [ ]\n",
    "    for i in range(1, 11):\n",
    "        url = ROOT_URL % i\n",
    "        # print(url)\n",
    "        # get html text from url using BeautifulSoup\n",
    "        # parse the html text to get the information you need\n",
    "        # save the information to a csv file\n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        # Your string\n",
    "        print(i, url,  len(soup.text))\n",
    "        string_data = soup.find_all('script')[1].text.split('\\n')[1][13:-1]\n",
    "\n",
    "        # Convert the string to list of dictionaries\n",
    "        data_list = json.loads(string_data)\n",
    "\n",
    "        # Print the list of dictionaries\n",
    "        print(data_list)\n",
    "        all_pages.append(data_list)\n",
    "    return all_pages\n",
    "    \n",
    "all_pages = crawl_arxiv_sanity_lite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 HTML\n",
      "1 \n",
      "\n",
      "2 <html>\n",
      "<head>\n",
      "<!-- meta info -->\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<title>arxiv-sanity</title>\n",
      "<!-- CSS -->\n",
      "<link href=\"/static/style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<!-- Favicon -->\n",
      "<link href=\"/static/favicon.png\" rel=\"shortcut icon\" type=\"image/png\">\n",
      "<script>\n",
      "var user = \"\";\n",
      "</script>\n",
      "<script>\n",
      "var papers = [{\"authors\": \"Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d\\u0027Autume, Dani Yogatama, Mikel Artetxe, Yi Tay\", \"id\": \"2405.02287\", \"summary\": \"We introduce Vibe-Eval: a new open benchmark and framework for evaluating\\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\\nincluding 100 of hard difficulty, complete with gold-standard responses\\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\\n(ii) rigorously testing and probing the capabilities of present frontier\\nmodels. Notably, our hard set contains \\u003e50% questions that all frontier models\\nanswer incorrectly. We explore the nuances of designing, evaluating, and\\nranking models on ultra challenging prompts. We also discuss trade-offs between\\nhuman and automatic evaluation, and show that automatic model evaluation using\\nReka Core roughly correlates to human judgment. We offer free API access for\\nthe purpose of lightweight evaluation and plan to conduct formal human\\nevaluations for public models that perform well on the Vibe-Eval\\u0027s automatic\\nscores. We release the evaluation code and data, see\\nhttps://github.com/reka-ai/reka-vibe-eval\", \"tags\": \"cs.CL, cs.AI, cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\\n  language models\", \"utags\": [], \"weight\": 2.6113746360413455}, {\"authors\": \"Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki\", \"id\": \"2405.02280\", \"summary\": \"Existing VLMs can track in-the-wild 2D video objects while current generative\\nmodels provide powerful visual priors for synthesizing novel views for the\\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\\nprogress, we present DreamScene4D, the first approach that can generate\\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\\nvideos with large object motion across occlusions and novel viewpoints. Our key\\ninsight is to design a \\\"decompose-then-recompose\\\" scheme to factorize both the\\nwhole video scene and each object\\u0027s 3D motion. We first decompose the video\\nscene by using open-vocabulary mask trackers and an adapted image diffusion\\nmodel to segment, track, and amodally complete the objects and background in\\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\\nmove in space and time. We also factorize the observed motion into multiple\\ncomponents to handle fast motion. The camera motion can be inferred by\\nre-rendering the background to match the video frames. For the object motion,\\nwe first model the object-centric deformation of the objects by leveraging\\nrendering losses and multi-view generative priors in an object-centric frame,\\nthen optimize object-centric to world-frame transformations by comparing the\\nrendered outputs against the perceived pixel and optical flow. Finally, we\\nrecompose the background and objects and optimize for relative object scales\\nusing monocular depth prediction guidance. We show extensive results on the\\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\\nand provide future directions. Besides 4D scene generation, our results show\\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\\ninferred 3D trajectories to 2D, while never explicitly trained to do so.\", \"tags\": \"cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\\n  Videos\", \"utags\": [], \"weight\": 2.6143954693746787}, {\"authors\": \"Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue\", \"id\": \"2405.00332\", \"summary\": \"Large language models (LLMs) have achieved impressive success on many\\nbenchmarks for mathematical reasoning. However, there is growing concern that\\nsome of this performance actually reflects dataset contamination, where data\\nclosely resembling benchmark questions leaks into the training data, instead of\\ntrue reasoning ability. To investigate this claim rigorously, we commission\\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\\nelementary mathematical reasoning. We ensure that the two benchmarks are\\ncomparable across important metrics such as human solve rates, number of steps\\nin solution, answer magnitude, and more. When evaluating leading open- and\\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\\nseveral families of models (e.g., Phi and Mistral) showing evidence of\\nsystematic overfitting across almost all model sizes. At the same time, many\\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\\nminimal signs of overfitting. Further analysis suggests a positive relationship\\n(Spearman\\u0027s r^2=0.32) between a model\\u0027s probability of generating an example\\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\\nmany models may have partially memorized GSM8k.\", \"tags\": \"cs.CL, cs.AI, cs.LG\", \"thumb_url\": \"static/thumb/2405.00332.jpg\", \"time\": \"May 03 2024\", \"title\": \"A Careful Examination of Large Language Model Performance on Grade\\n  School Arithmetic\", \"utags\": [], \"weight\": 2.6158769508561606}, {\"authors\": \"Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu\", \"id\": \"2309.12284\", \"summary\": \"Large language models (LLMs) have pushed the limits of natural language\\nunderstanding and exhibited excellent problem-solving ability. Despite the\\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\\naway from satisfactory for solving mathematical problem due to the complex\\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\\nlanguage model that specializes in mathematical reasoning. Specifically, we\\nstart by bootstrapping mathematical questions by rewriting the question from\\nmultiple perspectives without extra knowledge, which results in a new dataset\\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\\nMetaMathQA dataset, the MetaMath models with different model sizes and the\\ntraining code for public use.\", \"tags\": \"cs.CL, cs.AI\", \"thumb_url\": \"static/thumb/2309.12284.jpg\", \"time\": \"May 03 2024\", \"title\": \"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\\n  Models\", \"utags\": [], \"weight\": 2.627902413819123}, {\"authors\": \"Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, Rohit Babbar\", \"id\": \"2109.07319\", \"summary\": \"Automatic annotation of short-text data to a large number of target labels,\\nreferred to as Short Text Extreme Classification, has found numerous\\napplications including prediction of related searches and product\\nrecommendation. In this paper, we propose a convolutional architecture\\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\\nlack of word-order in short-text queries encountered in search and\\nrecommendation. We demonstrate the efficacy of applying convolutions by\\nrecasting the operation along the embedding dimension instead of the word\\ndimension as applied in conventional CNNs for text classification. Towards\\nscaling our model to datasets with millions of labels, we also propose SyncXML\\npipeline which improves upon the shortcomings of the recently proposed dynamic\\nhard-negative mining technique for label short-listing by synchronizing the\\nlabel-shortlister and extreme classifier. SyncXML not only reduces the\\ninference time to half but is also an order of magnitude smaller than\\nstate-of-the-art Astec in terms of model size. Through a comprehensive\\nempirical comparison, we show that not only can InceptionXML outperform\\nexisting approaches on benchmark datasets but also the transformer baselines\\nrequiring only 2% FLOPs. The code for InceptionXML is available at\\nhttps://github.com/xmc-aalto/inceptionxml.\", \"tags\": \"cs.CL, cs.AI, cs.LG\", \"thumb_url\": \"static/thumb/2109.07319.jpg\", \"time\": \"May 03 2024\", \"title\": \"InceptionXML: A Lightweight Framework with Synchronized Negative\\n  Sampling for Short Text Extreme Classification\", \"utags\": [], \"weight\": 2.628654728633938}, {\"authors\": \"Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau\", \"id\": \"2405.02267\", \"summary\": \"Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\\nstate-of-the-art for natural language understanding task when fine-tuned on\\nlabeled data. However, their large size poses challenges in deploying them for\\ninference in real-world applications, due to significant GPU memory\\nrequirements and high inference latency. This paper explores neural\\narchitecture search (NAS) for structural pruning to find sub-parts of the\\nfine-tuned network that optimally trade-off efficiency, for example in terms of\\nmodel size or latency, and generalization performance. We also show how we can\\nutilize more recently developed two-stage weight-sharing NAS approaches in this\\nsetting to accelerate the search process. Unlike traditional pruning methods\\nwith fixed thresholds, we propose to adopt a multi-objective approach that\\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\\nand automated compression process.\", \"tags\": \"cs.LG, cs.CL\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Structural Pruning of Pre-trained Language Models via Neural\\n  Architecture Search\", \"utags\": [], \"weight\": 2.628712599004308}, {\"authors\": \"Maxime Zanella, Ismail Ben Ayed\", \"id\": \"2405.02266\", \"summary\": \"The development of large vision-language models, notably CLIP, has catalyzed\\nresearch into effective adaptation techniques, with a particular focus on soft\\nprompt tuning. Conjointly, test-time augmentation, which utilizes multiple\\naugmented views of a single image to enhance zero-shot generalization, is\\nemerging as a significant area of interest. This has predominantly directed\\nresearch efforts toward test-time prompt tuning. In contrast, we introduce a\\nrobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based\\nmethods without requiring this intensive training procedure. This positions MTA\\nas an ideal solution for both standalone and API-based applications.\\nAdditionally, our method does not rely on ad hoc rules (e.g., confidence\\nthreshold) used in some previous test-time augmentation techniques to filter\\nthe augmented views. Instead, MTA incorporates a quality assessment variable\\nfor each view directly into its optimization process, termed as the inlierness\\nscore. This score is jointly optimized with a density mode seeking process,\\nleading to an efficient training- and hyperparameter-free approach. We\\nextensively benchmark our method on 15 datasets and demonstrate MTA\\u0027s\\nsuperiority and computational efficiency. Deployed easily as plug-and-play\\nmodule on top of zero-shot models and state-of-the-art few-shot methods, MTA\\nshows systematic and consistent improvements.\", \"tags\": \"cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"On the test-time zero-shot generalization of vision-language models: Do\\n  we really need prompt learning?\", \"utags\": [], \"weight\": 2.6293491730783827}, {\"authors\": \"Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou\", \"id\": \"2405.01327\", \"summary\": \"Existing studies on constrained reinforcement learning (RL) may obtain a\\nwell-performing policy in the training environment. However, when deployed in a\\nreal environment, it may easily violate constraints that were originally\\nsatisfied during training because there might be model mismatch between the\\ntraining and real environments. To address the above challenge, we formulate\\nthe problem as constrained RL under model uncertainty, where the goal is to\\nlearn a good policy that optimizes the reward and at the same time satisfy the\\nconstraint under model mismatch. We develop a Robust Constrained Policy\\nOptimization (RCPO) algorithm, which is the first algorithm that applies to\\nlarge/continuous state space and has theoretical guarantees on worst-case\\nreward improvement and constraint violation at each iteration during the\\ntraining. We demonstrate the effectiveness of our algorithm on a set of RL\\ntasks with constraints.\", \"tags\": \"cs.LG\", \"thumb_url\": \"static/thumb/2405.01327.jpg\", \"time\": \"May 03 2024\", \"title\": \"Constrained Reinforcement Learning Under Model Mismatch\", \"utags\": [], \"weight\": 2.6361894508561603}, {\"authors\": \"Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang\", \"id\": \"2402.11319\", \"summary\": \"Flexible continuum manipulators are valued for minimally invasive surgery,\\noffering access to confined spaces through nonlinear paths. However,\\ncable-driven manipulators face control difficulties due to hysteresis from\\ncabling effects such as friction, elongation, and coupling. These effects are\\ndifficult to model due to nonlinearity and the difficulties become even more\\nevident when dealing with long and coupled, multi-segmented manipulator. This\\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\\ncapture these nonlinear and previous states-dependent characteristics of cable\\nactuation. We collect physical joint configurations according to command joint\\nconfigurations using RGBD sensing and 7 fiducial markers to model the\\nhysteresis of the proposed manipulator. Result on a study comparing the\\nestimation performance of four DNN models show that the Temporal Convolution\\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\\nTracking tests in task space using unseen trajectories show that the proposed\\ncontrol algorithm reduces the average position and orientation error by 61.39%\\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\\\deg} to 11.21{\\\\deg}),\\nrespectively. This result implies that the proposed calibrated controller\\neffectively reaches the desired configurations by estimating the hysteresis of\\nthe manipulator. Applying this method in real surgical scenarios has the\\npotential to enhance control precision and improve surgical performance.\", \"tags\": \"cs.RO, cs.AI\", \"thumb_url\": \"static/thumb/2402.11319.jpg\", \"time\": \"May 03 2024\", \"title\": \"Hysteresis Compensation of Flexible Continuum Manipulator using RGBD\\n  Sensing and Temporal Convolutional Network\", \"utags\": [], \"weight\": 2.639430191596901}, {\"authors\": \"Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe\", \"id\": \"2310.04420\", \"summary\": \"Understanding the functional organization of higher visual cortex is a\\ncentral focus in neuroscience. Past studies have primarily mapped the visual\\nand semantic selectivity of neural populations using hand-selected stimuli,\\nwhich may potentially bias results towards pre-existing hypotheses of visual\\ncortex functionality. Moving beyond conventional approaches, we introduce a\\ndata-driven method that generates natural language descriptions for images\\npredicted to maximally activate individual voxels of interest. Our method --\\nSemantic Captioning Using Brain Alignments (\\\"BrainSCUBA\\\") -- builds upon the\\nrich embedding space learned by a contrastive vision-language model and\\nutilizes a pre-trained large language model to generate interpretable captions.\\nWe validate our method through fine-grained voxel-level captioning across\\nhigher-order visual regions. We further perform text-conditioned image\\nsynthesis with the captions, and show that our images are semantically coherent\\nand yield high predicted activations. Finally, to demonstrate how our method\\nenables scientific discovery, we perform exploratory investigations on the\\ndistribution of \\\"person\\\" representations in the brain, and discover\\nfine-grained semantic selectivity in body-selective areas. Unlike earlier\\nstudies that decode text, our method derives voxel-wise captions of semantic\\nselectivity. Our results show that BrainSCUBA is a promising means for\\nunderstanding functional preferences in the brain, and provides motivation for\\nfurther hypothesis-driven investigation of visual cortex.\", \"tags\": \"cs.LG, q-bio.NC\", \"thumb_url\": \"static/thumb/2310.04420.jpg\", \"time\": \"May 03 2024\", \"title\": \"BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex\\n  Selectivity\", \"utags\": [], \"weight\": 2.639765839745049}, {\"authors\": \"Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria\", \"id\": \"2310.07626\", \"summary\": \"Satellite-based remote sensing missions have revolutionized our understanding\\nof the Ocean state and dynamics. Among them, space-borne altimetry provides\\nvaluable measurements of Sea Surface Height (SSH), which is used to estimate\\nsurface geostrophic currents. Due to the sensor technology employed, important\\ngaps occur in SSH observations. Complete SSH maps are produced using linear\\nOptimal Interpolations (OI) such as the widely-used Data Unification and\\nAltimeter Combination System (duacs). On the other hand, Sea Surface\\nTemperature (SST) products have much higher data coverage and SST is physically\\nlinked to geostrophic currents through advection. We propose a new\\nmulti-variate Observing System Simulation Experiment (OSSE) emulating 20 years\\nof SSH and SST satellite observations. We train an Attention-Based\\nEncoder-Decoder deep learning network (abed) on this data, comparing two\\nsettings: one with access to ground truth during training and one without. On\\nour OSSE, we compare abed reconstructions when trained using either supervised\\nor unsupervised loss functions, with or without SST information. We evaluate\\nthe SSH interpolations in terms of eddy detection. We also introduce a new way\\nto transfer the learning from simulation to observations by doing a supervised\\npre-training on our OSSE followed by an unsupervised fine-tuning on satellite\\ndata. On real SSH observations from the Ocean Data Challenge 2021, we find that\\nthis learning strategy combined with the use of SST leads to a decrease of 24%\\nof the root mean squared error compared to duacs.\", \"tags\": \"cs.LG\", \"thumb_url\": \"static/thumb/2310.07626.jpg\", \"time\": \"May 03 2024\", \"title\": \"Learning of Sea Surface Height Interpolation from Multi-variate\\n  Simulated Satellite Observations\", \"utags\": [], \"weight\": 2.6445112101154193}, {\"authors\": \"Karl Van Wyk, Ankur Handa, Viktor Makoviychuk, Yijie Guo, Arthur Allshire, Nathan D. Ratliff\", \"id\": \"2405.02250\", \"summary\": \"Robotics policies are always subjected to complex, second order dynamics that\\nentangle their actions with resulting states. In reinforcement learning (RL)\\ncontexts, policies have the burden of deciphering these complicated\\ninteractions over massive amounts of experience and complex reward functions to\\nlearn how to accomplish tasks. Moreover, policies typically issue actions\\ndirectly to controllers like Operational Space Control (OSC) or joint PD\\ncontrol, which induces straightline motion towards these action targets in task\\nor joint space. However, straightline motion in these spaces for the most part\\ndo not capture the rich, nonlinear behavior our robots need to exhibit,\\nshifting the burden of discovering these behaviors more completely to the\\nagent. Unlike these simpler controllers, geometric fabrics capture a much\\nricher and desirable set of behaviors via artificial, second order dynamics\\ngrounded in nonlinear geometry. These artificial dynamics shift the\\nuncontrolled dynamics of a robot via an appropriate control law to form\\nbehavioral dynamics. Behavioral dynamics unlock a new action space and safe,\\nguiding behavior over which RL policies are trained. Behavioral dynamics enable\\nbang-bang-like RL policy actions that are still safe for real robots, simplify\\nreward engineering, and help sequence real-world, high-performance policies. We\\ndescribe the framework more generally and create a specific instantiation for\\nthe problem of dexterous, in-hand reorientation of a cube by a highly actuated\\nrobot hand.\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Geometric Fabrics: a Safe Guiding Medium for Policy Learning\", \"utags\": [], \"weight\": 2.6476014878931973}, {\"authors\": \"Hugo Lauren\\u00e7on, L\\u00e9o Tronchon, Matthieu Cord, Victor Sanh\", \"id\": \"2405.02246\", \"summary\": \"The growing interest in vision-language models (VLMs) has been driven by\\nimprovements in large language models and vision transformers. Despite the\\nabundance of literature on this subject, we observe that critical decisions\\nregarding the design of VLMs are often not justified. We argue that these\\nunsupported decisions impede progress in the field by making it difficult to\\nidentify which choices improve model performance. To address this issue, we\\nconduct extensive experiments around pre-trained models, architecture choice,\\ndata, and training methods. Our consolidation of findings includes the\\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\\nIdefics2 achieves state-of-the-art performance within its size category across\\nvarious multimodal benchmarks, and is often on par with models four times its\\nsize. We release the model (base, instructed, and chat) along with the datasets\\ncreated for its training.\", \"tags\": \"cs.CV, cs.AI\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"What matters when building vision-language models?\", \"utags\": [], \"weight\": 2.6529834323376416}, {\"authors\": \"Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, Anders Johannsen\", \"id\": \"2403.00462\", \"summary\": \"Spurred by recent advances in Large Language Models (LLMs), virtual\\nassistants are poised to take a leap forward in terms of their dialogue\\ncapabilities. Yet a major bottleneck to achieving genuinely transformative\\ntask-oriented dialogue capabilities remains the scarcity of high quality data.\\nExisting datasets, while impressive in scale, have limited domain coverage and\\ncontain few genuinely challenging conversational phenomena; those which are\\npresent are typically unlabelled, making it difficult to assess the strengths\\nand weaknesses of models without time-consuming and costly human evaluation.\\nMoreover, creating high quality dialogue data has until now required\\nconsiderable human input, limiting both the scale of these datasets and the\\nability to rapidly bootstrap data for a new target domain. We aim to overcome\\nthese issues with LUCID, a modularised and highly automated LLM-driven data\\ngeneration system that produces realistic, diverse and challenging dialogues.\\nWe use LUCID to generate a seed dataset of 4,277 conversations across 100\\nintents to demonstrate its capabilities, with a human review finding\\nconsistently high quality labels in the generated data.\", \"tags\": \"cs.CL, I.2.7\", \"thumb_url\": \"static/thumb/2403.00462.jpg\", \"time\": \"May 03 2024\", \"title\": \"LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues\", \"utags\": [], \"weight\": 2.6549857471524567}, {\"authors\": \"Hanwen Qi, Edward Sun, Harry Zhang\", \"id\": \"2405.02243\", \"summary\": \"Behavioral cloning, or more broadly, learning from demonstrations (LfD) is a\\npriomising direction for robot policy learning in complex scenarios. Albeit\\nbeing straightforward to implement and data-efficient, behavioral cloning has\\nits own drawbacks, limiting its efficacy in real robot setups. In this work, we\\ntake one step towards improving learning from demonstration algorithms by\\nleveraging implicit energy-based policy models. Results suggest that in\\nselected complex robot policy learning scenarios, treating supervised policy\\nlearning with an implicit model generally performs better, on average, than\\ncommonly used neural network-based explicit models, especially in the cases of\\napproximating potentially discontinuous and multimodal functions.\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Towards Improving Learning from Demonstration Algorithms via MCMC\\n  Methods\", \"utags\": [], \"weight\": 2.657566765670975}, {\"authors\": \"Xuxin Cheng, Heng Yu, Harry Zhang, Wenxing Deng\", \"id\": \"2405.02241\", \"summary\": \"We present a novel method for robotic manipulation tasks in human\\nenvironments that require reasoning about the 3D geometric relationship between\\na pair of objects. Traditional end-to-end trained policies, which map from\\npixel observations to low-level robot actions, struggle to reason about complex\\npose relationships and have difficulty generalizing to unseen object\\nconfigurations. To address these challenges, we propose a method that learns to\\nreason about the 3D geometric relationship between objects, focusing on the\\nrelationship between key parts on one object with respect to key parts on\\nanother object. Our standalone model utilizes Weighted SVD to reason about both\\npose relationships between articulated parts and between free-floating objects.\\nThis approach allows the robot to understand the relationship between the oven\\ndoor and the oven body, as well as the relationship between the lasagna plate\\nand the oven, for example. By considering the 3D geometric relationship between\\nobjects, our method enables robots to perform complex manipulation tasks that\\nreason about object-centric representations. We open source the code and\\ndemonstrate the results here\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD\", \"utags\": [], \"weight\": 2.6585274138191233}, {\"authors\": \"Elika Bozorgi, Saber Soleimani, Sakher Khalil Alqaiidi, Hamid Reza Arabnia, Krzysztof Kochut\", \"id\": \"2405.02240\", \"summary\": \"Graph is an important data representation which occurs naturally in the real\\nworld applications \\\\cite{goyal2018graph}. Therefore, analyzing graphs provides\\nusers with better insights in different areas such as anomaly detection\\n\\\\cite{ma2021comprehensive}, decision making \\\\cite{fan2023graph}, clustering\\n\\\\cite{tsitsulin2023graph}, classification \\\\cite{wang2021mixup} and etc.\\nHowever, most of these methods require high levels of computational time and\\nspace. We can use other ways like embedding to reduce these costs. Knowledge\\ngraph (KG) embedding is a technique that aims to achieve the vector\\nrepresentation of a KG. It represents entities and relations of a KG in a\\nlow-dimensional space while maintaining the semantic meanings of them. There\\nare different methods for embedding graphs including random walk-based methods\\nsuch as node2vec, metapath2vec and regpattern2vec. However, most of these\\nmethods bias the walks based on a rigid pattern usually hard-coded in the\\nalgorithm. In this work, we introduce \\\\textit{subgraph2vec} for embedding KGs\\nwhere walks are run inside a user-defined subgraph. We use this embedding for\\nlink prediction and prove our method has better performance in most cases in\\ncomparison with the previous ones.\", \"tags\": \"cs.LG\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Subgraph2vec: A random walk-based algorithm for embedding knowledge\\n  graphs\", \"utags\": [], \"weight\": 2.6590250990043085}, {\"authors\": \"Yongxin Zhou, Fabien Ringeval, Fran\\u00e7ois Portet\", \"id\": \"2307.12371\", \"summary\": \"Automatic dialogue summarization is a well-established task with the goal of\\ndistilling the most crucial information from human conversations into concise\\ntextual summaries. However, most existing research has predominantly focused on\\nsummarizing factual information, neglecting the affective content, which can\\nhold valuable insights for analyzing, monitoring, or facilitating human\\ninteractions. In this paper, we introduce and assess a set of measures\\nPSentScore, aimed at quantifying the preservation of affective content in\\ndialogue summaries. Our findings indicate that state-of-the-art summarization\\nmodels do not preserve well the affective content within their summaries.\\nMoreover, we demonstrate that a careful selection of the training set for\\ndialogue samples can lead to improved preservation of affective content in the\\ngenerated summaries, albeit with a minor reduction in content-related metrics.\", \"tags\": \"cs.CL\", \"thumb_url\": \"static/thumb/2307.12371.jpg\", \"time\": \"May 03 2024\", \"title\": \"PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization\", \"utags\": [], \"weight\": 2.660738061967271}, {\"authors\": \"Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil\", \"id\": \"2312.06733\", \"summary\": \"LiDAR Upsampling is a challenging task for the perception systems of robots\\nand autonomous vehicles, due to the sparse and irregular structure of\\nlarge-scale scene contexts. Recent works propose to solve this problem by\\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\\nproblem in 2D image space. Although their methods can generate high-resolution\\nrange images with fine-grained details, the resulting 3D point clouds often\\nblur out details and predict invalid points. In this paper, we propose TULIP, a\\nnew method to reconstruct high-resolution LiDAR point clouds from\\nlow-resolution LiDAR input. We also follow a range image-based approach but\\nspecifically modify the patch and window geometries of a Swin-Transformer-based\\nnetwork to better fit the characteristics of range images. We conducted several\\nexperiments on three public real-world and simulated datasets. TULIP\\noutperforms state-of-the-art methods in all relevant metrics and generates\\nrobust and more realistic point clouds than prior works.\", \"tags\": \"cs.CV\", \"thumb_url\": \"static/thumb/2312.06733.jpg\", \"time\": \"May 03 2024\", \"title\": \"TULIP: Transformer for Upsampling of LiDAR Point Clouds\", \"utags\": [], \"weight\": 2.6620459323376418}, {\"authors\": \"Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley\", \"id\": \"2310.09653\", \"summary\": \"We propose SelfVC, a training strategy to iteratively improve a voice\\nconversion model with self-synthesized examples. Previous efforts on voice\\nconversion focus on factorizing speech into explicitly disentangled\\nrepresentations that separately encode speaker characteristics and linguistic\\ncontent. However, disentangling speech representations to capture such\\nattributes using task-specific loss terms can lead to information loss. In this\\nwork, instead of explicitly disentangling attributes with loss terms, we\\npresent a framework to train a controllable voice conversion model on entangled\\nspeech representations derived from self-supervised learning (SSL) and speaker\\nverification models. First, we develop techniques to derive prosodic\\ninformation from the audio signal and SSL representations to train predictive\\nsubmodules in the synthesis model. Next, we propose a training strategy to\\niteratively improve the synthesis model for voice conversion, by creating a\\nchallenging training objective using self-synthesized examples. We demonstrate\\nthat incorporating such self-synthesized examples during training improves the\\nspeaker similarity of generated speech as compared to a baseline voice\\nconversion model trained solely on heuristically perturbed inputs. Our\\nframework is trained without any text and achieves state-of-the-art results in\\nzero-shot voice conversion on metrics evaluating naturalness, speaker\\nsimilarity, and intelligibility of synthesized audio.\", \"tags\": \"cs.SD, cs.AI, eess.AS\", \"thumb_url\": \"static/thumb/2310.09653.jpg\", \"time\": \"May 03 2024\", \"title\": \"SelfVC: Voice Conversion With Iterative Refinement using Self\\n  Transformations\", \"utags\": [], \"weight\": 2.6629487101154194}, {\"authors\": \"Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, Matteo Papini\", \"id\": \"2405.02235\", \"summary\": \"Policy gradient (PG) methods are successful approaches to deal with\\ncontinuous reinforcement learning (RL) problems. They learn stochastic\\nparametric (hyper)policies by either exploring in the space of actions or in\\nthe space of parameters. Stochastic controllers, however, are often undesirable\\nfrom a practical perspective because of their lack of robustness, safety, and\\ntraceability. In common practice, stochastic (hyper)policies are learned only\\nto deploy their deterministic version. In this paper, we make a step towards\\nthe theoretical understanding of this practice. After introducing a novel\\nframework for modeling this scenario, we study the global convergence to the\\nbest deterministic policy, under (weak) gradient domination assumptions. Then,\\nwe illustrate how to tune the exploration level used for learning to optimize\\nthe trade-off between the sample complexity and the performance of the deployed\\ndeterministic policy. Finally, we quantitatively compare action-based and\\nparameter-based exploration, giving a formal guise to intuitive results.\", \"tags\": \"cs.LG\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Learning Optimal Deterministic Policies with Stochastic Policy Gradients\", \"utags\": [], \"weight\": 2.6632264878931973}, {\"authors\": \"Qiqi Su, Christos Kloukinas, Artur d\\u0027Avila Garcez\", \"id\": \"2311.16834\", \"summary\": \"Multivariate time series have many applications, from healthcare and\\nmeteorology to life science. Although deep learning models have shown excellent\\npredictive performance for time series, they have been criticised for being\\n\\\"black-boxes\\\" or non-interpretable. This paper proposes a novel modular neural\\nnetwork model for multivariate time series prediction that is interpretable by\\nconstruction. A recurrent neural network learns the temporal dependencies in\\nthe data while an attention-based feature selection component selects the most\\nrelevant features and suppresses redundant features used in the learning of the\\ntemporal dependencies. A modular deep network is trained from the selected\\nfeatures independently to show the users how features influence outcomes,\\nmaking the model interpretable. Experimental results show that this approach\\ncan outperform state-of-the-art interpretable Neural Additive Models (NAM) and\\nvariations thereof in both regression and classification of time series tasks,\\nachieving a predictive performance that is comparable to the top\\nnon-interpretable methods for time series, LSTM and XGBoost.\", \"tags\": \"cs.LG, cs.AI\", \"thumb_url\": \"static/thumb/2311.16834.jpg\", \"time\": \"May 03 2024\", \"title\": \"FocusLearn: Fully-Interpretable, High-Performance Modular Neural\\n  Networks for Time Series\", \"utags\": [], \"weight\": 2.6637357471524568}, {\"authors\": \"Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur\", \"id\": \"2405.02228\", \"summary\": \"Automatic citation generation for sentences in a document or report is\\nparamount for intelligence analysts, cybersecurity, news agencies, and\\neducation personnel. In this research, we investigate whether large language\\nmodels (LLMs) are capable of generating references based on two forms of\\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\\nthe title of a mentioned article when given a sentence from a different\\narticle. To demonstrate where LLM stands in this task, we introduce a large\\ndataset called REASONS comprising abstracts of the 12 most popular domains of\\nscientific research on arXiv. From around 20K research articles, we make the\\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\\npercentage (PP) to minimize the hallucination rate (HR). When tested with\\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\\nHR across all domains and models decreased by an average of 41.93% and the PP\\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\\nstruggle to understand context, but the extent of this issue was small in\\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\\nreliability of RAG for automated citation generation tasks.\", \"tags\": \"cs.CL, cs.AI, cs.IR\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\\n  Sentences using Public and Proprietary LLMs\", \"utags\": [], \"weight\": 2.6676709323376415}, {\"authors\": \"Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen\", \"id\": \"2405.01461\", \"summary\": \"Is the Text to Motion model robust? Recent advancements in Text to Motion\\nmodels primarily stem from more accurate predictions of specific actions.\\nHowever, the text modality typically relies solely on pre-trained Contrastive\\nLanguage-Image Pretraining (CLIP) models. Our research has uncovered a\\nsignificant issue with the text-to-motion model: its predictions often exhibit\\ninconsistent outputs, resulting in vastly different or even incorrect poses\\nwhen presented with semantically similar or identical text inputs. In this\\npaper, we undertake an analysis to elucidate the underlying causes of this\\ninstability, establishing a clear link between the unpredictability of model\\noutputs and the erratic attention patterns of the text encoder module.\\nConsequently, we introduce a formal framework aimed at addressing this issue,\\nwhich we term the Stable Text-to-Motion Framework (SATO). SATO consists of\\nthree modules, each dedicated to stable attention, stable prediction, and\\nmaintaining a balance between accuracy and robustness trade-off. We present a\\nmethodology for constructing an SATO that satisfies the stability of attention\\nand prediction. To verify the stability of the model, we introduced a new\\ntextual synonym perturbation dataset based on HumanML3D and KIT-ML. Results\\nshow that SATO is significantly more stable against synonyms and other slight\\nperturbations while keeping its high accuracy performance.\", \"tags\": \"cs.CV\", \"thumb_url\": \"static/thumb/2405.01461.jpg\", \"time\": \"May 03 2024\", \"title\": \"SATO: Stable Text-to-Motion Framework\", \"utags\": [], \"weight\": 2.6698352841894937}, {\"authors\": \"Lujing Zhang, Aaron Roth, Linjun Zhang\", \"id\": \"2405.02225\", \"summary\": \"This paper introduces a framework for post-processing machine learning models\\nso that their predictions satisfy multi-group fairness guarantees. Based on the\\ncelebrated notion of multicalibration, we introduce $(\\\\mathbf{s},\\\\mathcal{G},\\n\\\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\\nmulti-dimensional mappings $\\\\mathbf{s}$, constraint set $\\\\mathcal{G}$, and a\\npre-specified threshold level $\\\\alpha$. We propose associated algorithms to\\nachieve this notion in general settings. This framework is then applied to\\ndiverse scenarios encompassing different fairness concerns, including false\\nnegative rate control in image segmentation, prediction set conditional\\nuncertainty quantification in hierarchical classification, and de-biased text\\ngeneration in language models. We conduct numerical studies on several datasets\\nand tasks.\", \"tags\": \"stat.ML, cs.AI, cs.CY, cs.LG, stat.ME\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Fair Risk Control: A Generalized Framework for Calibrating Multi-group\\n  Fairness Risks\", \"utags\": [], \"weight\": 2.6723237101154194}];\n",
      "var tags = [];\n",
      "var words = [];\n",
      "var words_desc = \"Here are the top 40 most positive and bottom 20 most negative weights of the SVM. If they don\\u0027t look great then try tuning the regularization strength hyperparameter of the SVM, svm_c, above. Lower C is higher regularization.\";\n",
      "var gvars = {\"page_number\": \"1\", \"pid\": \"\", \"rank\": \"time\", \"search_query\": \"\", \"skip_have\": \"no\", \"svm_c\": \"0.01\", \"tags\": \"\", \"time_filter\": \"\"};\n",
      "\n",
      "/*\n",
      "JS code here handles pagination. I really don't super love this approach,\n",
      "if anyone can think of a cleaner / shorter way please let me know.\n",
      "*/\n",
      "var move_page = function(int_offset) {\n",
      "    var queryParams = new URLSearchParams(window.location.search);\n",
      "    queryParams.set(\"page_number\", Math.max(1, parseInt(gvars.page_number) + int_offset));\n",
      "    window.location.href = '/?' + queryParams.toString();\n",
      "}\n",
      "</script>\n",
      "</link></head>\n",
      "<body>\n",
      "<div id=\"header\">\n",
      "<a href=\"/\" id=\"home-link\">arxiv-sanity</a>\n",
      "<a href=\"/profile\" id=\"login-link\">\n",
      "        login\n",
      "    </a>\n",
      "<a href=\"/stats\" id=\"stats-link\">stats</a>\n",
      "<a href=\"/about\" id=\"stats-link\">about</a>\n",
      "</div>\n",
      "<div id=\"log-fun-warn\">(hi! just btw you have to be logged in to be able to add/delete/curate tags for papers and get recommendations)</div>\n",
      "<div id=\"controls\">\n",
      "<div>\n",
      "<!-- the choice box, allowing us to sort, rank, slice and dice papers -->\n",
      "<div id=\"cbox\">\n",
      "<form action=\"/\" method=\"get\">\n",
      "<!-- the search box, allowing us to search by keywords -->\n",
      "<input id=\"qfield\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "<!-- rank type: one of tags, pid, time, or random -->\n",
      "<label for=\"rank_type\">Rank by:</label>\n",
      "<select id=\"rank_select\" name=\"rank\">\n",
      "<option false=\"\" value=\"search\">search</option>\n",
      "<option false=\"\" value=\"tags\">tags</option>\n",
      "<option false=\"\" value=\"pid\">pid</option>\n",
      "<option selected=\"\" value=\"time\">time</option>\n",
      "<option false=\"\" value=\"random\">random</option>\n",
      "</select>\n",
      "<!-- current tags, simply in a text field -->\n",
      "<label for=\"tags\">tags: </label>\n",
      "<input id=\"tags_field\" name=\"tags\" type=\"text\" value=\"\"/>\n",
      "<!-- current pid, simply in a text field -->\n",
      "<label for=\"pid\">pid: </label>\n",
      "<input id=\"pid_field\" name=\"pid\" type=\"text\" value=\"\"/>\n",
      "<!-- current time_filter, in a text field -->\n",
      "<label for=\"time_filter\">time_filter (days): </label>\n",
      "<input id=\"time_filter_field\" name=\"time_filter\" type=\"text\" value=\"\"/>\n",
      "<!-- current svm_c, in a text field -->\n",
      "<label for=\"svm_c\">svm_c: </label>\n",
      "<input id=\"svm_c_field\" name=\"svm_c\" type=\"text\" value=\"0.01\"/>\n",
      "<!-- current skip_have: one of yes or no -->\n",
      "<label for=\"skip_have\">skip_have: </label>\n",
      "<select id=\"skip_have_select\" name=\"skip_have\">\n",
      "<option false=\"\" value=\"yes\">yes</option>\n",
      "<option selected=\"\" value=\"no\">no</option>\n",
      "</select>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "</div>\n",
      "<!-- some hand-coded common choices for faster and more convenient operation -->\n",
      "<div id=\"cbox_fast\">\n",
      "            Shortcuts:\n",
      "            <a href=\"/?rank=tags&amp;tags=all&amp;time_filter=7&amp;skip_have=yes\">recommend over last week</a>\n",
      "<a href=\"/?rank=tags&amp;tags=all&amp;time_filter=3&amp;skip_have=yes\">recommend over last 3 days</a>\n",
      "<a href=\"/?rank=time\">recent</a>\n",
      "<a href=\"/?rank=random&amp;time_filter=7\">random last week</a>\n",
      "</div>\n",
      "</div>\n",
      "<div>\n",
      "</div>\n",
      "</div>\n",
      "<!-- main content showing all the papers as a list -->\n",
      "<div id=\"wrap\">\n",
      "</div>\n",
      "<!-- links to previous and next pages -->\n",
      "<div id=\"pagination\">\n",
      "<span id=\"link-prev-page\" onclick=\"move_page(-1);\">prev</span>\n",
      "<span>current page: 1 </span>\n",
      "<span id=\"link-next-page\" onclick=\"move_page(1);\">next</span>\n",
      "</div>\n",
      "<!-- React -->\n",
      "<script crossorigin=\"\" src=\"https://unpkg.com/react@16/umd/react.production.min.js\"></script>\n",
      "<script crossorigin=\"\" src=\"https://unpkg.com/react-dom@16/umd/react-dom.production.min.js\"></script>\n",
      "<!-- Babel for displaying JSX -->\n",
      "<script src=\"https://unpkg.com/babel-standalone@6/babel.min.js\"></script>\n",
      "<!-- Load our React component -->\n",
      "<script src=\"/static/paper_list.js\" type=\"text/babel\"></script>\n",
      "<script src=\"/static/word_list.js\" type=\"text/babel\"></script>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# aa\n",
    "# get the data from var papers = [] in aa\n",
    "# get the line has \"var papers\" in aa\n",
    "for idl, line in enumerate(aa):\n",
    "    print(idl, line)\n",
    "    # if \"var papers\" in line:\n",
    "    #     print(idl)\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"authors\": \"Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d\\\\u0027Autume, Dani Yogatama, Mikel Artetxe, Yi Tay\", \"id\": \"2405.02287\", \"summary\": \"We introduce Vibe-Eval: a new open benchmark and framework for evaluating\\\\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\\\\nincluding 100 of hard difficulty, complete with gold-standard responses\\\\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\\\\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\\\\n(ii) rigorously testing and probing the capabilities of present frontier\\\\nmodels. Notably, our hard set contains \\\\u003e50% questions that all frontier models\\\\nanswer incorrectly. We explore the nuances of designing, evaluating, and\\\\nranking models on ultra challenging prompts. We also discuss trade-offs between\\\\nhuman and automatic evaluation, and show that automatic model evaluation using\\\\nReka Core roughly correlates to human judgment. We offer free API access for\\\\nthe purpose of lightweight evaluation and plan to conduct formal human\\\\nevaluations for public models that perform well on the Vibe-Eval\\\\u0027s automatic\\\\nscores. We release the evaluation code and data, see\\\\nhttps://github.com/reka-ai/reka-vibe-eval\", \"tags\": \"cs.CL, cs.AI, cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\\\\n  language models\", \"utags\": [], \"weight\": 2.6113746360413455}, {\"authors\": \"Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki\", \"id\": \"2405.02280\", \"summary\": \"Existing VLMs can track in-the-wild 2D video objects while current generative\\\\nmodels provide powerful visual priors for synthesizing novel views for the\\\\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\\\\nprogress, we present DreamScene4D, the first approach that can generate\\\\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\\\\nvideos with large object motion across occlusions and novel viewpoints. Our key\\\\ninsight is to design a \\\\\"decompose-then-recompose\\\\\" scheme to factorize both the\\\\nwhole video scene and each object\\\\u0027s 3D motion. We first decompose the video\\\\nscene by using open-vocabulary mask trackers and an adapted image diffusion\\\\nmodel to segment, track, and amodally complete the objects and background in\\\\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\\\\nmove in space and time. We also factorize the observed motion into multiple\\\\ncomponents to handle fast motion. The camera motion can be inferred by\\\\nre-rendering the background to match the video frames. For the object motion,\\\\nwe first model the object-centric deformation of the objects by leveraging\\\\nrendering losses and multi-view generative priors in an object-centric frame,\\\\nthen optimize object-centric to world-frame transformations by comparing the\\\\nrendered outputs against the perceived pixel and optical flow. Finally, we\\\\nrecompose the background and objects and optimize for relative object scales\\\\nusing monocular depth prediction guidance. We show extensive results on the\\\\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\\\\nand provide future directions. Besides 4D scene generation, our results show\\\\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\\\\ninferred 3D trajectories to 2D, while never explicitly trained to do so.\", \"tags\": \"cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\\\\n  Videos\", \"utags\": [], \"weight\": 2.6143954693746787}, {\"authors\": \"Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue\", \"id\": \"2405.00332\", \"summary\": \"Large language models (LLMs) have achieved impressive success on many\\\\nbenchmarks for mathematical reasoning. However, there is growing concern that\\\\nsome of this performance actually reflects dataset contamination, where data\\\\nclosely resembling benchmark questions leaks into the training data, instead of\\\\ntrue reasoning ability. To investigate this claim rigorously, we commission\\\\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\\\\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\\\\nelementary mathematical reasoning. We ensure that the two benchmarks are\\\\ncomparable across important metrics such as human solve rates, number of steps\\\\nin solution, answer magnitude, and more. When evaluating leading open- and\\\\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\\\\nseveral families of models (e.g., Phi and Mistral) showing evidence of\\\\nsystematic overfitting across almost all model sizes. At the same time, many\\\\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\\\\nminimal signs of overfitting. Further analysis suggests a positive relationship\\\\n(Spearman\\\\u0027s r^2=0.32) between a model\\\\u0027s probability of generating an example\\\\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\\\\nmany models may have partially memorized GSM8k.\", \"tags\": \"cs.CL, cs.AI, cs.LG\", \"thumb_url\": \"static/thumb/2405.00332.jpg\", \"time\": \"May 03 2024\", \"title\": \"A Careful Examination of Large Language Model Performance on Grade\\\\n  School Arithmetic\", \"utags\": [], \"weight\": 2.6158769508561606}, {\"authors\": \"Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu\", \"id\": \"2309.12284\", \"summary\": \"Large language models (LLMs) have pushed the limits of natural language\\\\nunderstanding and exhibited excellent problem-solving ability. Despite the\\\\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\\\\naway from satisfactory for solving mathematical problem due to the complex\\\\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\\\\nlanguage model that specializes in mathematical reasoning. Specifically, we\\\\nstart by bootstrapping mathematical questions by rewriting the question from\\\\nmultiple perspectives without extra knowledge, which results in a new dataset\\\\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\\\\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\\\\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\\\\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\\\\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\\\\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\\\\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\\\\nMetaMathQA dataset, the MetaMath models with different model sizes and the\\\\ntraining code for public use.\", \"tags\": \"cs.CL, cs.AI\", \"thumb_url\": \"static/thumb/2309.12284.jpg\", \"time\": \"May 03 2024\", \"title\": \"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\\\\n  Models\", \"utags\": [], \"weight\": 2.627902413819123}, {\"authors\": \"Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, Rohit Babbar\", \"id\": \"2109.07319\", \"summary\": \"Automatic annotation of short-text data to a large number of target labels,\\\\nreferred to as Short Text Extreme Classification, has found numerous\\\\napplications including prediction of related searches and product\\\\nrecommendation. In this paper, we propose a convolutional architecture\\\\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\\\\nlack of word-order in short-text queries encountered in search and\\\\nrecommendation. We demonstrate the efficacy of applying convolutions by\\\\nrecasting the operation along the embedding dimension instead of the word\\\\ndimension as applied in conventional CNNs for text classification. Towards\\\\nscaling our model to datasets with millions of labels, we also propose SyncXML\\\\npipeline which improves upon the shortcomings of the recently proposed dynamic\\\\nhard-negative mining technique for label short-listing by synchronizing the\\\\nlabel-shortlister and extreme classifier. SyncXML not only reduces the\\\\ninference time to half but is also an order of magnitude smaller than\\\\nstate-of-the-art Astec in terms of model size. Through a comprehensive\\\\nempirical comparison, we show that not only can InceptionXML outperform\\\\nexisting approaches on benchmark datasets but also the transformer baselines\\\\nrequiring only 2% FLOPs. The code for InceptionXML is available at\\\\nhttps://github.com/xmc-aalto/inceptionxml.\", \"tags\": \"cs.CL, cs.AI, cs.LG\", \"thumb_url\": \"static/thumb/2109.07319.jpg\", \"time\": \"May 03 2024\", \"title\": \"InceptionXML: A Lightweight Framework with Synchronized Negative\\\\n  Sampling for Short Text Extreme Classification\", \"utags\": [], \"weight\": 2.628654728633938}, {\"authors\": \"Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau\", \"id\": \"2405.02267\", \"summary\": \"Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\\\\nstate-of-the-art for natural language understanding task when fine-tuned on\\\\nlabeled data. However, their large size poses challenges in deploying them for\\\\ninference in real-world applications, due to significant GPU memory\\\\nrequirements and high inference latency. This paper explores neural\\\\narchitecture search (NAS) for structural pruning to find sub-parts of the\\\\nfine-tuned network that optimally trade-off efficiency, for example in terms of\\\\nmodel size or latency, and generalization performance. We also show how we can\\\\nutilize more recently developed two-stage weight-sharing NAS approaches in this\\\\nsetting to accelerate the search process. Unlike traditional pruning methods\\\\nwith fixed thresholds, we propose to adopt a multi-objective approach that\\\\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\\\\nand automated compression process.\", \"tags\": \"cs.LG, cs.CL\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Structural Pruning of Pre-trained Language Models via Neural\\\\n  Architecture Search\", \"utags\": [], \"weight\": 2.628712599004308}, {\"authors\": \"Maxime Zanella, Ismail Ben Ayed\", \"id\": \"2405.02266\", \"summary\": \"The development of large vision-language models, notably CLIP, has catalyzed\\\\nresearch into effective adaptation techniques, with a particular focus on soft\\\\nprompt tuning. Conjointly, test-time augmentation, which utilizes multiple\\\\naugmented views of a single image to enhance zero-shot generalization, is\\\\nemerging as a significant area of interest. This has predominantly directed\\\\nresearch efforts toward test-time prompt tuning. In contrast, we introduce a\\\\nrobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based\\\\nmethods without requiring this intensive training procedure. This positions MTA\\\\nas an ideal solution for both standalone and API-based applications.\\\\nAdditionally, our method does not rely on ad hoc rules (e.g., confidence\\\\nthreshold) used in some previous test-time augmentation techniques to filter\\\\nthe augmented views. Instead, MTA incorporates a quality assessment variable\\\\nfor each view directly into its optimization process, termed as the inlierness\\\\nscore. This score is jointly optimized with a density mode seeking process,\\\\nleading to an efficient training- and hyperparameter-free approach. We\\\\nextensively benchmark our method on 15 datasets and demonstrate MTA\\\\u0027s\\\\nsuperiority and computational efficiency. Deployed easily as plug-and-play\\\\nmodule on top of zero-shot models and state-of-the-art few-shot methods, MTA\\\\nshows systematic and consistent improvements.\", \"tags\": \"cs.CV\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"On the test-time zero-shot generalization of vision-language models: Do\\\\n  we really need prompt learning?\", \"utags\": [], \"weight\": 2.6293491730783827}, {\"authors\": \"Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou\", \"id\": \"2405.01327\", \"summary\": \"Existing studies on constrained reinforcement learning (RL) may obtain a\\\\nwell-performing policy in the training environment. However, when deployed in a\\\\nreal environment, it may easily violate constraints that were originally\\\\nsatisfied during training because there might be model mismatch between the\\\\ntraining and real environments. To address the above challenge, we formulate\\\\nthe problem as constrained RL under model uncertainty, where the goal is to\\\\nlearn a good policy that optimizes the reward and at the same time satisfy the\\\\nconstraint under model mismatch. We develop a Robust Constrained Policy\\\\nOptimization (RCPO) algorithm, which is the first algorithm that applies to\\\\nlarge/continuous state space and has theoretical guarantees on worst-case\\\\nreward improvement and constraint violation at each iteration during the\\\\ntraining. We demonstrate the effectiveness of our algorithm on a set of RL\\\\ntasks with constraints.\", \"tags\": \"cs.LG\", \"thumb_url\": \"static/thumb/2405.01327.jpg\", \"time\": \"May 03 2024\", \"title\": \"Constrained Reinforcement Learning Under Model Mismatch\", \"utags\": [], \"weight\": 2.6361894508561603}, {\"authors\": \"Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang\", \"id\": \"2402.11319\", \"summary\": \"Flexible continuum manipulators are valued for minimally invasive surgery,\\\\noffering access to confined spaces through nonlinear paths. However,\\\\ncable-driven manipulators face control difficulties due to hysteresis from\\\\ncabling effects such as friction, elongation, and coupling. These effects are\\\\ndifficult to model due to nonlinearity and the difficulties become even more\\\\nevident when dealing with long and coupled, multi-segmented manipulator. This\\\\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\\\\ncapture these nonlinear and previous states-dependent characteristics of cable\\\\nactuation. We collect physical joint configurations according to command joint\\\\nconfigurations using RGBD sensing and 7 fiducial markers to model the\\\\nhysteresis of the proposed manipulator. Result on a study comparing the\\\\nestimation performance of four DNN models show that the Temporal Convolution\\\\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\\\\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\\\\nTracking tests in task space using unseen trajectories show that the proposed\\\\ncontrol algorithm reduces the average position and orientation error by 61.39%\\\\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\\\\\\\deg} to 11.21{\\\\\\\\deg}),\\\\nrespectively. This result implies that the proposed calibrated controller\\\\neffectively reaches the desired configurations by estimating the hysteresis of\\\\nthe manipulator. Applying this method in real surgical scenarios has the\\\\npotential to enhance control precision and improve surgical performance.\", \"tags\": \"cs.RO, cs.AI\", \"thumb_url\": \"static/thumb/2402.11319.jpg\", \"time\": \"May 03 2024\", \"title\": \"Hysteresis Compensation of Flexible Continuum Manipulator using RGBD\\\\n  Sensing and Temporal Convolutional Network\", \"utags\": [], \"weight\": 2.639430191596901}, {\"authors\": \"Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe\", \"id\": \"2310.04420\", \"summary\": \"Understanding the functional organization of higher visual cortex is a\\\\ncentral focus in neuroscience. Past studies have primarily mapped the visual\\\\nand semantic selectivity of neural populations using hand-selected stimuli,\\\\nwhich may potentially bias results towards pre-existing hypotheses of visual\\\\ncortex functionality. Moving beyond conventional approaches, we introduce a\\\\ndata-driven method that generates natural language descriptions for images\\\\npredicted to maximally activate individual voxels of interest. Our method --\\\\nSemantic Captioning Using Brain Alignments (\\\\\"BrainSCUBA\\\\\") -- builds upon the\\\\nrich embedding space learned by a contrastive vision-language model and\\\\nutilizes a pre-trained large language model to generate interpretable captions.\\\\nWe validate our method through fine-grained voxel-level captioning across\\\\nhigher-order visual regions. We further perform text-conditioned image\\\\nsynthesis with the captions, and show that our images are semantically coherent\\\\nand yield high predicted activations. Finally, to demonstrate how our method\\\\nenables scientific discovery, we perform exploratory investigations on the\\\\ndistribution of \\\\\"person\\\\\" representations in the brain, and discover\\\\nfine-grained semantic selectivity in body-selective areas. Unlike earlier\\\\nstudies that decode text, our method derives voxel-wise captions of semantic\\\\nselectivity. Our results show that BrainSCUBA is a promising means for\\\\nunderstanding functional preferences in the brain, and provides motivation for\\\\nfurther hypothesis-driven investigation of visual cortex.\", \"tags\": \"cs.LG, q-bio.NC\", \"thumb_url\": \"static/thumb/2310.04420.jpg\", \"time\": \"May 03 2024\", \"title\": \"BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex\\\\n  Selectivity\", \"utags\": [], \"weight\": 2.639765839745049}, {\"authors\": \"Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria\", \"id\": \"2310.07626\", \"summary\": \"Satellite-based remote sensing missions have revolutionized our understanding\\\\nof the Ocean state and dynamics. Among them, space-borne altimetry provides\\\\nvaluable measurements of Sea Surface Height (SSH), which is used to estimate\\\\nsurface geostrophic currents. Due to the sensor technology employed, important\\\\ngaps occur in SSH observations. Complete SSH maps are produced using linear\\\\nOptimal Interpolations (OI) such as the widely-used Data Unification and\\\\nAltimeter Combination System (duacs). On the other hand, Sea Surface\\\\nTemperature (SST) products have much higher data coverage and SST is physically\\\\nlinked to geostrophic currents through advection. We propose a new\\\\nmulti-variate Observing System Simulation Experiment (OSSE) emulating 20 years\\\\nof SSH and SST satellite observations. We train an Attention-Based\\\\nEncoder-Decoder deep learning network (abed) on this data, comparing two\\\\nsettings: one with access to ground truth during training and one without. On\\\\nour OSSE, we compare abed reconstructions when trained using either supervised\\\\nor unsupervised loss functions, with or without SST information. We evaluate\\\\nthe SSH interpolations in terms of eddy detection. We also introduce a new way\\\\nto transfer the learning from simulation to observations by doing a supervised\\\\npre-training on our OSSE followed by an unsupervised fine-tuning on satellite\\\\ndata. On real SSH observations from the Ocean Data Challenge 2021, we find that\\\\nthis learning strategy combined with the use of SST leads to a decrease of 24%\\\\nof the root mean squared error compared to duacs.\", \"tags\": \"cs.LG\", \"thumb_url\": \"static/thumb/2310.07626.jpg\", \"time\": \"May 03 2024\", \"title\": \"Learning of Sea Surface Height Interpolation from Multi-variate\\\\n  Simulated Satellite Observations\", \"utags\": [], \"weight\": 2.6445112101154193}, {\"authors\": \"Karl Van Wyk, Ankur Handa, Viktor Makoviychuk, Yijie Guo, Arthur Allshire, Nathan D. Ratliff\", \"id\": \"2405.02250\", \"summary\": \"Robotics policies are always subjected to complex, second order dynamics that\\\\nentangle their actions with resulting states. In reinforcement learning (RL)\\\\ncontexts, policies have the burden of deciphering these complicated\\\\ninteractions over massive amounts of experience and complex reward functions to\\\\nlearn how to accomplish tasks. Moreover, policies typically issue actions\\\\ndirectly to controllers like Operational Space Control (OSC) or joint PD\\\\ncontrol, which induces straightline motion towards these action targets in task\\\\nor joint space. However, straightline motion in these spaces for the most part\\\\ndo not capture the rich, nonlinear behavior our robots need to exhibit,\\\\nshifting the burden of discovering these behaviors more completely to the\\\\nagent. Unlike these simpler controllers, geometric fabrics capture a much\\\\nricher and desirable set of behaviors via artificial, second order dynamics\\\\ngrounded in nonlinear geometry. These artificial dynamics shift the\\\\nuncontrolled dynamics of a robot via an appropriate control law to form\\\\nbehavioral dynamics. Behavioral dynamics unlock a new action space and safe,\\\\nguiding behavior over which RL policies are trained. Behavioral dynamics enable\\\\nbang-bang-like RL policy actions that are still safe for real robots, simplify\\\\nreward engineering, and help sequence real-world, high-performance policies. We\\\\ndescribe the framework more generally and create a specific instantiation for\\\\nthe problem of dexterous, in-hand reorientation of a cube by a highly actuated\\\\nrobot hand.\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Geometric Fabrics: a Safe Guiding Medium for Policy Learning\", \"utags\": [], \"weight\": 2.6476014878931973}, {\"authors\": \"Hugo Lauren\\\\u00e7on, L\\\\u00e9o Tronchon, Matthieu Cord, Victor Sanh\", \"id\": \"2405.02246\", \"summary\": \"The growing interest in vision-language models (VLMs) has been driven by\\\\nimprovements in large language models and vision transformers. Despite the\\\\nabundance of literature on this subject, we observe that critical decisions\\\\nregarding the design of VLMs are often not justified. We argue that these\\\\nunsupported decisions impede progress in the field by making it difficult to\\\\nidentify which choices improve model performance. To address this issue, we\\\\nconduct extensive experiments around pre-trained models, architecture choice,\\\\ndata, and training methods. Our consolidation of findings includes the\\\\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\\\\nIdefics2 achieves state-of-the-art performance within its size category across\\\\nvarious multimodal benchmarks, and is often on par with models four times its\\\\nsize. We release the model (base, instructed, and chat) along with the datasets\\\\ncreated for its training.\", \"tags\": \"cs.CV, cs.AI\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"What matters when building vision-language models?\", \"utags\": [], \"weight\": 2.6529834323376416}, {\"authors\": \"Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, Anders Johannsen\", \"id\": \"2403.00462\", \"summary\": \"Spurred by recent advances in Large Language Models (LLMs), virtual\\\\nassistants are poised to take a leap forward in terms of their dialogue\\\\ncapabilities. Yet a major bottleneck to achieving genuinely transformative\\\\ntask-oriented dialogue capabilities remains the scarcity of high quality data.\\\\nExisting datasets, while impressive in scale, have limited domain coverage and\\\\ncontain few genuinely challenging conversational phenomena; those which are\\\\npresent are typically unlabelled, making it difficult to assess the strengths\\\\nand weaknesses of models without time-consuming and costly human evaluation.\\\\nMoreover, creating high quality dialogue data has until now required\\\\nconsiderable human input, limiting both the scale of these datasets and the\\\\nability to rapidly bootstrap data for a new target domain. We aim to overcome\\\\nthese issues with LUCID, a modularised and highly automated LLM-driven data\\\\ngeneration system that produces realistic, diverse and challenging dialogues.\\\\nWe use LUCID to generate a seed dataset of 4,277 conversations across 100\\\\nintents to demonstrate its capabilities, with a human review finding\\\\nconsistently high quality labels in the generated data.\", \"tags\": \"cs.CL, I.2.7\", \"thumb_url\": \"static/thumb/2403.00462.jpg\", \"time\": \"May 03 2024\", \"title\": \"LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues\", \"utags\": [], \"weight\": 2.6549857471524567}, {\"authors\": \"Hanwen Qi, Edward Sun, Harry Zhang\", \"id\": \"2405.02243\", \"summary\": \"Behavioral cloning, or more broadly, learning from demonstrations (LfD) is a\\\\npriomising direction for robot policy learning in complex scenarios. Albeit\\\\nbeing straightforward to implement and data-efficient, behavioral cloning has\\\\nits own drawbacks, limiting its efficacy in real robot setups. In this work, we\\\\ntake one step towards improving learning from demonstration algorithms by\\\\nleveraging implicit energy-based policy models. Results suggest that in\\\\nselected complex robot policy learning scenarios, treating supervised policy\\\\nlearning with an implicit model generally performs better, on average, than\\\\ncommonly used neural network-based explicit models, especially in the cases of\\\\napproximating potentially discontinuous and multimodal functions.\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Towards Improving Learning from Demonstration Algorithms via MCMC\\\\n  Methods\", \"utags\": [], \"weight\": 2.657566765670975}, {\"authors\": \"Xuxin Cheng, Heng Yu, Harry Zhang, Wenxing Deng\", \"id\": \"2405.02241\", \"summary\": \"We present a novel method for robotic manipulation tasks in human\\\\nenvironments that require reasoning about the 3D geometric relationship between\\\\na pair of objects. Traditional end-to-end trained policies, which map from\\\\npixel observations to low-level robot actions, struggle to reason about complex\\\\npose relationships and have difficulty generalizing to unseen object\\\\nconfigurations. To address these challenges, we propose a method that learns to\\\\nreason about the 3D geometric relationship between objects, focusing on the\\\\nrelationship between key parts on one object with respect to key parts on\\\\nanother object. Our standalone model utilizes Weighted SVD to reason about both\\\\npose relationships between articulated parts and between free-floating objects.\\\\nThis approach allows the robot to understand the relationship between the oven\\\\ndoor and the oven body, as well as the relationship between the lasagna plate\\\\nand the oven, for example. By considering the 3D geometric relationship between\\\\nobjects, our method enables robots to perform complex manipulation tasks that\\\\nreason about object-centric representations. We open source the code and\\\\ndemonstrate the results here\", \"tags\": \"cs.RO\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD\", \"utags\": [], \"weight\": 2.6585274138191233}, {\"authors\": \"Elika Bozorgi, Saber Soleimani, Sakher Khalil Alqaiidi, Hamid Reza Arabnia, Krzysztof Kochut\", \"id\": \"2405.02240\", \"summary\": \"Graph is an important data representation which occurs naturally in the real\\\\nworld applications \\\\\\\\cite{goyal2018graph}. Therefore, analyzing graphs provides\\\\nusers with better insights in different areas such as anomaly detection\\\\n\\\\\\\\cite{ma2021comprehensive}, decision making \\\\\\\\cite{fan2023graph}, clustering\\\\n\\\\\\\\cite{tsitsulin2023graph}, classification \\\\\\\\cite{wang2021mixup} and etc.\\\\nHowever, most of these methods require high levels of computational time and\\\\nspace. We can use other ways like embedding to reduce these costs. Knowledge\\\\ngraph (KG) embedding is a technique that aims to achieve the vector\\\\nrepresentation of a KG. It represents entities and relations of a KG in a\\\\nlow-dimensional space while maintaining the semantic meanings of them. There\\\\nare different methods for embedding graphs including random walk-based methods\\\\nsuch as node2vec, metapath2vec and regpattern2vec. However, most of these\\\\nmethods bias the walks based on a rigid pattern usually hard-coded in the\\\\nalgorithm. In this work, we introduce \\\\\\\\textit{subgraph2vec} for embedding KGs\\\\nwhere walks are run inside a user-defined subgraph. We use this embedding for\\\\nlink prediction and prove our method has better performance in most cases in\\\\ncomparison with the previous ones.\", \"tags\": \"cs.LG\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Subgraph2vec: A random walk-based algorithm for embedding knowledge\\\\n  graphs\", \"utags\": [], \"weight\": 2.6590250990043085}, {\"authors\": \"Yongxin Zhou, Fabien Ringeval, Fran\\\\u00e7ois Portet\", \"id\": \"2307.12371\", \"summary\": \"Automatic dialogue summarization is a well-established task with the goal of\\\\ndistilling the most crucial information from human conversations into concise\\\\ntextual summaries. However, most existing research has predominantly focused on\\\\nsummarizing factual information, neglecting the affective content, which can\\\\nhold valuable insights for analyzing, monitoring, or facilitating human\\\\ninteractions. In this paper, we introduce and assess a set of measures\\\\nPSentScore, aimed at quantifying the preservation of affective content in\\\\ndialogue summaries. Our findings indicate that state-of-the-art summarization\\\\nmodels do not preserve well the affective content within their summaries.\\\\nMoreover, we demonstrate that a careful selection of the training set for\\\\ndialogue samples can lead to improved preservation of affective content in the\\\\ngenerated summaries, albeit with a minor reduction in content-related metrics.\", \"tags\": \"cs.CL\", \"thumb_url\": \"static/thumb/2307.12371.jpg\", \"time\": \"May 03 2024\", \"title\": \"PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization\", \"utags\": [], \"weight\": 2.660738061967271}, {\"authors\": \"Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil\", \"id\": \"2312.06733\", \"summary\": \"LiDAR Upsampling is a challenging task for the perception systems of robots\\\\nand autonomous vehicles, due to the sparse and irregular structure of\\\\nlarge-scale scene contexts. Recent works propose to solve this problem by\\\\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\\\\nproblem in 2D image space. Although their methods can generate high-resolution\\\\nrange images with fine-grained details, the resulting 3D point clouds often\\\\nblur out details and predict invalid points. In this paper, we propose TULIP, a\\\\nnew method to reconstruct high-resolution LiDAR point clouds from\\\\nlow-resolution LiDAR input. We also follow a range image-based approach but\\\\nspecifically modify the patch and window geometries of a Swin-Transformer-based\\\\nnetwork to better fit the characteristics of range images. We conducted several\\\\nexperiments on three public real-world and simulated datasets. TULIP\\\\noutperforms state-of-the-art methods in all relevant metrics and generates\\\\nrobust and more realistic point clouds than prior works.\", \"tags\": \"cs.CV\", \"thumb_url\": \"static/thumb/2312.06733.jpg\", \"time\": \"May 03 2024\", \"title\": \"TULIP: Transformer for Upsampling of LiDAR Point Clouds\", \"utags\": [], \"weight\": 2.6620459323376418}, {\"authors\": \"Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley\", \"id\": \"2310.09653\", \"summary\": \"We propose SelfVC, a training strategy to iteratively improve a voice\\\\nconversion model with self-synthesized examples. Previous efforts on voice\\\\nconversion focus on factorizing speech into explicitly disentangled\\\\nrepresentations that separately encode speaker characteristics and linguistic\\\\ncontent. However, disentangling speech representations to capture such\\\\nattributes using task-specific loss terms can lead to information loss. In this\\\\nwork, instead of explicitly disentangling attributes with loss terms, we\\\\npresent a framework to train a controllable voice conversion model on entangled\\\\nspeech representations derived from self-supervised learning (SSL) and speaker\\\\nverification models. First, we develop techniques to derive prosodic\\\\ninformation from the audio signal and SSL representations to train predictive\\\\nsubmodules in the synthesis model. Next, we propose a training strategy to\\\\niteratively improve the synthesis model for voice conversion, by creating a\\\\nchallenging training objective using self-synthesized examples. We demonstrate\\\\nthat incorporating such self-synthesized examples during training improves the\\\\nspeaker similarity of generated speech as compared to a baseline voice\\\\nconversion model trained solely on heuristically perturbed inputs. Our\\\\nframework is trained without any text and achieves state-of-the-art results in\\\\nzero-shot voice conversion on metrics evaluating naturalness, speaker\\\\nsimilarity, and intelligibility of synthesized audio.\", \"tags\": \"cs.SD, cs.AI, eess.AS\", \"thumb_url\": \"static/thumb/2310.09653.jpg\", \"time\": \"May 03 2024\", \"title\": \"SelfVC: Voice Conversion With Iterative Refinement using Self\\\\n  Transformations\", \"utags\": [], \"weight\": 2.6629487101154194}, {\"authors\": \"Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, Matteo Papini\", \"id\": \"2405.02235\", \"summary\": \"Policy gradient (PG) methods are successful approaches to deal with\\\\ncontinuous reinforcement learning (RL) problems. They learn stochastic\\\\nparametric (hyper)policies by either exploring in the space of actions or in\\\\nthe space of parameters. Stochastic controllers, however, are often undesirable\\\\nfrom a practical perspective because of their lack of robustness, safety, and\\\\ntraceability. In common practice, stochastic (hyper)policies are learned only\\\\nto deploy their deterministic version. In this paper, we make a step towards\\\\nthe theoretical understanding of this practice. After introducing a novel\\\\nframework for modeling this scenario, we study the global convergence to the\\\\nbest deterministic policy, under (weak) gradient domination assumptions. Then,\\\\nwe illustrate how to tune the exploration level used for learning to optimize\\\\nthe trade-off between the sample complexity and the performance of the deployed\\\\ndeterministic policy. Finally, we quantitatively compare action-based and\\\\nparameter-based exploration, giving a formal guise to intuitive results.\", \"tags\": \"cs.LG\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Learning Optimal Deterministic Policies with Stochastic Policy Gradients\", \"utags\": [], \"weight\": 2.6632264878931973}, {\"authors\": \"Qiqi Su, Christos Kloukinas, Artur d\\\\u0027Avila Garcez\", \"id\": \"2311.16834\", \"summary\": \"Multivariate time series have many applications, from healthcare and\\\\nmeteorology to life science. Although deep learning models have shown excellent\\\\npredictive performance for time series, they have been criticised for being\\\\n\\\\\"black-boxes\\\\\" or non-interpretable. This paper proposes a novel modular neural\\\\nnetwork model for multivariate time series prediction that is interpretable by\\\\nconstruction. A recurrent neural network learns the temporal dependencies in\\\\nthe data while an attention-based feature selection component selects the most\\\\nrelevant features and suppresses redundant features used in the learning of the\\\\ntemporal dependencies. A modular deep network is trained from the selected\\\\nfeatures independently to show the users how features influence outcomes,\\\\nmaking the model interpretable. Experimental results show that this approach\\\\ncan outperform state-of-the-art interpretable Neural Additive Models (NAM) and\\\\nvariations thereof in both regression and classification of time series tasks,\\\\nachieving a predictive performance that is comparable to the top\\\\nnon-interpretable methods for time series, LSTM and XGBoost.\", \"tags\": \"cs.LG, cs.AI\", \"thumb_url\": \"static/thumb/2311.16834.jpg\", \"time\": \"May 03 2024\", \"title\": \"FocusLearn: Fully-Interpretable, High-Performance Modular Neural\\\\n  Networks for Time Series\", \"utags\": [], \"weight\": 2.6637357471524568}, {\"authors\": \"Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur\", \"id\": \"2405.02228\", \"summary\": \"Automatic citation generation for sentences in a document or report is\\\\nparamount for intelligence analysts, cybersecurity, news agencies, and\\\\neducation personnel. In this research, we investigate whether large language\\\\nmodels (LLMs) are capable of generating references based on two forms of\\\\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\\\\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\\\\nthe title of a mentioned article when given a sentence from a different\\\\narticle. To demonstrate where LLM stands in this task, we introduce a large\\\\ndataset called REASONS comprising abstracts of the 12 most popular domains of\\\\nscientific research on arXiv. From around 20K research articles, we make the\\\\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\\\\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\\\\npercentage (PP) to minimize the hallucination rate (HR). When tested with\\\\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\\\\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\\\\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\\\\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\\\\nHR across all domains and models decreased by an average of 41.93% and the PP\\\\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\\\\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\\\\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\\\\nstruggle to understand context, but the extent of this issue was small in\\\\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\\\\nreliability of RAG for automated citation generation tasks.\", \"tags\": \"cs.CL, cs.AI, cs.IR\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\\\\n  Sentences using Public and Proprietary LLMs\", \"utags\": [], \"weight\": 2.6676709323376415}, {\"authors\": \"Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen\", \"id\": \"2405.01461\", \"summary\": \"Is the Text to Motion model robust? Recent advancements in Text to Motion\\\\nmodels primarily stem from more accurate predictions of specific actions.\\\\nHowever, the text modality typically relies solely on pre-trained Contrastive\\\\nLanguage-Image Pretraining (CLIP) models. Our research has uncovered a\\\\nsignificant issue with the text-to-motion model: its predictions often exhibit\\\\ninconsistent outputs, resulting in vastly different or even incorrect poses\\\\nwhen presented with semantically similar or identical text inputs. In this\\\\npaper, we undertake an analysis to elucidate the underlying causes of this\\\\ninstability, establishing a clear link between the unpredictability of model\\\\noutputs and the erratic attention patterns of the text encoder module.\\\\nConsequently, we introduce a formal framework aimed at addressing this issue,\\\\nwhich we term the Stable Text-to-Motion Framework (SATO). SATO consists of\\\\nthree modules, each dedicated to stable attention, stable prediction, and\\\\nmaintaining a balance between accuracy and robustness trade-off. We present a\\\\nmethodology for constructing an SATO that satisfies the stability of attention\\\\nand prediction. To verify the stability of the model, we introduced a new\\\\ntextual synonym perturbation dataset based on HumanML3D and KIT-ML. Results\\\\nshow that SATO is significantly more stable against synonyms and other slight\\\\nperturbations while keeping its high accuracy performance.\", \"tags\": \"cs.CV\", \"thumb_url\": \"static/thumb/2405.01461.jpg\", \"time\": \"May 03 2024\", \"title\": \"SATO: Stable Text-to-Motion Framework\", \"utags\": [], \"weight\": 2.6698352841894937}, {\"authors\": \"Lujing Zhang, Aaron Roth, Linjun Zhang\", \"id\": \"2405.02225\", \"summary\": \"This paper introduces a framework for post-processing machine learning models\\\\nso that their predictions satisfy multi-group fairness guarantees. Based on the\\\\ncelebrated notion of multicalibration, we introduce $(\\\\\\\\mathbf{s},\\\\\\\\mathcal{G},\\\\n\\\\\\\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\\\\nmulti-dimensional mappings $\\\\\\\\mathbf{s}$, constraint set $\\\\\\\\mathcal{G}$, and a\\\\npre-specified threshold level $\\\\\\\\alpha$. We propose associated algorithms to\\\\nachieve this notion in general settings. This framework is then applied to\\\\ndiverse scenarios encompassing different fairness concerns, including false\\\\nnegative rate control in image segmentation, prediction set conditional\\\\nuncertainty quantification in hierarchical classification, and de-biased text\\\\ngeneration in language models. We conduct numerical studies on several datasets\\\\nand tasks.\", \"tags\": \"stat.ML, cs.AI, cs.CY, cs.LG, stat.ME\", \"thumb_url\": \"\", \"time\": \"May 03 2024\", \"title\": \"Fair Risk Control: A Generalized Framework for Calibrating Multi-group\\\\n  Fairness Risks\", \"utags\": [], \"weight\": 2.6723237101154194}];'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.find_all('script')[1].text.split('\\n')[1][13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 40580 (char 40579)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43maa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(cc)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 40580 (char 40579)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "cc = json.loads(aa.find_all('script')[1].text.split('\\n')[1][13:])\n",
    "len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'authors': \"Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Mikel Artetxe, Yi Tay\", 'id': '2405.02287', 'summary': \"We introduce Vibe-Eval: a new open benchmark and framework for evaluating\\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\\nincluding 100 of hard difficulty, complete with gold-standard responses\\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\\n(ii) rigorously testing and probing the capabilities of present frontier\\nmodels. Notably, our hard set contains >50% questions that all frontier models\\nanswer incorrectly. We explore the nuances of designing, evaluating, and\\nranking models on ultra challenging prompts. We also discuss trade-offs between\\nhuman and automatic evaluation, and show that automatic model evaluation using\\nReka Core roughly correlates to human judgment. We offer free API access for\\nthe purpose of lightweight evaluation and plan to conduct formal human\\nevaluations for public models that perform well on the Vibe-Eval's automatic\\nscores. We release the evaluation code and data, see\\nhttps://github.com/reka-ai/reka-vibe-eval\", 'tags': 'cs.CL, cs.AI, cs.CV', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\\n  language models', 'utags': [], 'weight': 2.6113746360413455}, {'authors': 'Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki', 'id': '2405.02280', 'summary': 'Existing VLMs can track in-the-wild 2D video objects while current generative\\nmodels provide powerful visual priors for synthesizing novel views for the\\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\\nprogress, we present DreamScene4D, the first approach that can generate\\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\\nvideos with large object motion across occlusions and novel viewpoints. Our key\\ninsight is to design a \"decompose-then-recompose\" scheme to factorize both the\\nwhole video scene and each object\\'s 3D motion. We first decompose the video\\nscene by using open-vocabulary mask trackers and an adapted image diffusion\\nmodel to segment, track, and amodally complete the objects and background in\\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\\nmove in space and time. We also factorize the observed motion into multiple\\ncomponents to handle fast motion. The camera motion can be inferred by\\nre-rendering the background to match the video frames. For the object motion,\\nwe first model the object-centric deformation of the objects by leveraging\\nrendering losses and multi-view generative priors in an object-centric frame,\\nthen optimize object-centric to world-frame transformations by comparing the\\nrendered outputs against the perceived pixel and optical flow. Finally, we\\nrecompose the background and objects and optimize for relative object scales\\nusing monocular depth prediction guidance. We show extensive results on the\\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\\nand provide future directions. Besides 4D scene generation, our results show\\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\\ninferred 3D trajectories to 2D, while never explicitly trained to do so.', 'tags': 'cs.CV', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\\n  Videos', 'utags': [], 'weight': 2.6143954693746787}, {'authors': 'Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue', 'id': '2405.00332', 'summary': \"Large language models (LLMs) have achieved impressive success on many\\nbenchmarks for mathematical reasoning. However, there is growing concern that\\nsome of this performance actually reflects dataset contamination, where data\\nclosely resembling benchmark questions leaks into the training data, instead of\\ntrue reasoning ability. To investigate this claim rigorously, we commission\\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\\nelementary mathematical reasoning. We ensure that the two benchmarks are\\ncomparable across important metrics such as human solve rates, number of steps\\nin solution, answer magnitude, and more. When evaluating leading open- and\\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\\nseveral families of models (e.g., Phi and Mistral) showing evidence of\\nsystematic overfitting across almost all model sizes. At the same time, many\\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\\nminimal signs of overfitting. Further analysis suggests a positive relationship\\n(Spearman's r^2=0.32) between a model's probability of generating an example\\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\\nmany models may have partially memorized GSM8k.\", 'tags': 'cs.CL, cs.AI, cs.LG', 'thumb_url': 'static/thumb/2405.00332.jpg', 'time': 'May 03 2024', 'title': 'A Careful Examination of Large Language Model Performance on Grade\\n  School Arithmetic', 'utags': [], 'weight': 2.6158769508561606}, {'authors': 'Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu', 'id': '2309.12284', 'summary': 'Large language models (LLMs) have pushed the limits of natural language\\nunderstanding and exhibited excellent problem-solving ability. Despite the\\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\\naway from satisfactory for solving mathematical problem due to the complex\\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\\nlanguage model that specializes in mathematical reasoning. Specifically, we\\nstart by bootstrapping mathematical questions by rewriting the question from\\nmultiple perspectives without extra knowledge, which results in a new dataset\\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\\nMetaMathQA dataset, the MetaMath models with different model sizes and the\\ntraining code for public use.', 'tags': 'cs.CL, cs.AI', 'thumb_url': 'static/thumb/2309.12284.jpg', 'time': 'May 03 2024', 'title': 'MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\\n  Models', 'utags': [], 'weight': 2.627902413819123}, {'authors': 'Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, Rohit Babbar', 'id': '2109.07319', 'summary': 'Automatic annotation of short-text data to a large number of target labels,\\nreferred to as Short Text Extreme Classification, has found numerous\\napplications including prediction of related searches and product\\nrecommendation. In this paper, we propose a convolutional architecture\\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\\nlack of word-order in short-text queries encountered in search and\\nrecommendation. We demonstrate the efficacy of applying convolutions by\\nrecasting the operation along the embedding dimension instead of the word\\ndimension as applied in conventional CNNs for text classification. Towards\\nscaling our model to datasets with millions of labels, we also propose SyncXML\\npipeline which improves upon the shortcomings of the recently proposed dynamic\\nhard-negative mining technique for label short-listing by synchronizing the\\nlabel-shortlister and extreme classifier. SyncXML not only reduces the\\ninference time to half but is also an order of magnitude smaller than\\nstate-of-the-art Astec in terms of model size. Through a comprehensive\\nempirical comparison, we show that not only can InceptionXML outperform\\nexisting approaches on benchmark datasets but also the transformer baselines\\nrequiring only 2% FLOPs. The code for InceptionXML is available at\\nhttps://github.com/xmc-aalto/inceptionxml.', 'tags': 'cs.CL, cs.AI, cs.LG', 'thumb_url': 'static/thumb/2109.07319.jpg', 'time': 'May 03 2024', 'title': 'InceptionXML: A Lightweight Framework with Synchronized Negative\\n  Sampling for Short Text Extreme Classification', 'utags': [], 'weight': 2.628654728633938}, {'authors': 'Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau', 'id': '2405.02267', 'summary': 'Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\\nstate-of-the-art for natural language understanding task when fine-tuned on\\nlabeled data. However, their large size poses challenges in deploying them for\\ninference in real-world applications, due to significant GPU memory\\nrequirements and high inference latency. This paper explores neural\\narchitecture search (NAS) for structural pruning to find sub-parts of the\\nfine-tuned network that optimally trade-off efficiency, for example in terms of\\nmodel size or latency, and generalization performance. We also show how we can\\nutilize more recently developed two-stage weight-sharing NAS approaches in this\\nsetting to accelerate the search process. Unlike traditional pruning methods\\nwith fixed thresholds, we propose to adopt a multi-objective approach that\\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\\nand automated compression process.', 'tags': 'cs.LG, cs.CL', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Structural Pruning of Pre-trained Language Models via Neural\\n  Architecture Search', 'utags': [], 'weight': 2.628712599004308}, {'authors': 'Maxime Zanella, Ismail Ben Ayed', 'id': '2405.02266', 'summary': \"The development of large vision-language models, notably CLIP, has catalyzed\\nresearch into effective adaptation techniques, with a particular focus on soft\\nprompt tuning. Conjointly, test-time augmentation, which utilizes multiple\\naugmented views of a single image to enhance zero-shot generalization, is\\nemerging as a significant area of interest. This has predominantly directed\\nresearch efforts toward test-time prompt tuning. In contrast, we introduce a\\nrobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based\\nmethods without requiring this intensive training procedure. This positions MTA\\nas an ideal solution for both standalone and API-based applications.\\nAdditionally, our method does not rely on ad hoc rules (e.g., confidence\\nthreshold) used in some previous test-time augmentation techniques to filter\\nthe augmented views. Instead, MTA incorporates a quality assessment variable\\nfor each view directly into its optimization process, termed as the inlierness\\nscore. This score is jointly optimized with a density mode seeking process,\\nleading to an efficient training- and hyperparameter-free approach. We\\nextensively benchmark our method on 15 datasets and demonstrate MTA's\\nsuperiority and computational efficiency. Deployed easily as plug-and-play\\nmodule on top of zero-shot models and state-of-the-art few-shot methods, MTA\\nshows systematic and consistent improvements.\", 'tags': 'cs.CV', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'On the test-time zero-shot generalization of vision-language models: Do\\n  we really need prompt learning?', 'utags': [], 'weight': 2.6293491730783827}, {'authors': 'Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou', 'id': '2405.01327', 'summary': 'Existing studies on constrained reinforcement learning (RL) may obtain a\\nwell-performing policy in the training environment. However, when deployed in a\\nreal environment, it may easily violate constraints that were originally\\nsatisfied during training because there might be model mismatch between the\\ntraining and real environments. To address the above challenge, we formulate\\nthe problem as constrained RL under model uncertainty, where the goal is to\\nlearn a good policy that optimizes the reward and at the same time satisfy the\\nconstraint under model mismatch. We develop a Robust Constrained Policy\\nOptimization (RCPO) algorithm, which is the first algorithm that applies to\\nlarge/continuous state space and has theoretical guarantees on worst-case\\nreward improvement and constraint violation at each iteration during the\\ntraining. We demonstrate the effectiveness of our algorithm on a set of RL\\ntasks with constraints.', 'tags': 'cs.LG', 'thumb_url': 'static/thumb/2405.01327.jpg', 'time': 'May 03 2024', 'title': 'Constrained Reinforcement Learning Under Model Mismatch', 'utags': [], 'weight': 2.6361894508561603}, {'authors': 'Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang', 'id': '2402.11319', 'summary': 'Flexible continuum manipulators are valued for minimally invasive surgery,\\noffering access to confined spaces through nonlinear paths. However,\\ncable-driven manipulators face control difficulties due to hysteresis from\\ncabling effects such as friction, elongation, and coupling. These effects are\\ndifficult to model due to nonlinearity and the difficulties become even more\\nevident when dealing with long and coupled, multi-segmented manipulator. This\\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\\ncapture these nonlinear and previous states-dependent characteristics of cable\\nactuation. We collect physical joint configurations according to command joint\\nconfigurations using RGBD sensing and 7 fiducial markers to model the\\nhysteresis of the proposed manipulator. Result on a study comparing the\\nestimation performance of four DNN models show that the Temporal Convolution\\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\\nTracking tests in task space using unseen trajectories show that the proposed\\ncontrol algorithm reduces the average position and orientation error by 61.39%\\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\\\deg} to 11.21{\\\\deg}),\\nrespectively. This result implies that the proposed calibrated controller\\neffectively reaches the desired configurations by estimating the hysteresis of\\nthe manipulator. Applying this method in real surgical scenarios has the\\npotential to enhance control precision and improve surgical performance.', 'tags': 'cs.RO, cs.AI', 'thumb_url': 'static/thumb/2402.11319.jpg', 'time': 'May 03 2024', 'title': 'Hysteresis Compensation of Flexible Continuum Manipulator using RGBD\\n  Sensing and Temporal Convolutional Network', 'utags': [], 'weight': 2.639430191596901}, {'authors': 'Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe', 'id': '2310.04420', 'summary': 'Understanding the functional organization of higher visual cortex is a\\ncentral focus in neuroscience. Past studies have primarily mapped the visual\\nand semantic selectivity of neural populations using hand-selected stimuli,\\nwhich may potentially bias results towards pre-existing hypotheses of visual\\ncortex functionality. Moving beyond conventional approaches, we introduce a\\ndata-driven method that generates natural language descriptions for images\\npredicted to maximally activate individual voxels of interest. Our method --\\nSemantic Captioning Using Brain Alignments (\"BrainSCUBA\") -- builds upon the\\nrich embedding space learned by a contrastive vision-language model and\\nutilizes a pre-trained large language model to generate interpretable captions.\\nWe validate our method through fine-grained voxel-level captioning across\\nhigher-order visual regions. We further perform text-conditioned image\\nsynthesis with the captions, and show that our images are semantically coherent\\nand yield high predicted activations. Finally, to demonstrate how our method\\nenables scientific discovery, we perform exploratory investigations on the\\ndistribution of \"person\" representations in the brain, and discover\\nfine-grained semantic selectivity in body-selective areas. Unlike earlier\\nstudies that decode text, our method derives voxel-wise captions of semantic\\nselectivity. Our results show that BrainSCUBA is a promising means for\\nunderstanding functional preferences in the brain, and provides motivation for\\nfurther hypothesis-driven investigation of visual cortex.', 'tags': 'cs.LG, q-bio.NC', 'thumb_url': 'static/thumb/2310.04420.jpg', 'time': 'May 03 2024', 'title': 'BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex\\n  Selectivity', 'utags': [], 'weight': 2.639765839745049}, {'authors': 'Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria', 'id': '2310.07626', 'summary': 'Satellite-based remote sensing missions have revolutionized our understanding\\nof the Ocean state and dynamics. Among them, space-borne altimetry provides\\nvaluable measurements of Sea Surface Height (SSH), which is used to estimate\\nsurface geostrophic currents. Due to the sensor technology employed, important\\ngaps occur in SSH observations. Complete SSH maps are produced using linear\\nOptimal Interpolations (OI) such as the widely-used Data Unification and\\nAltimeter Combination System (duacs). On the other hand, Sea Surface\\nTemperature (SST) products have much higher data coverage and SST is physically\\nlinked to geostrophic currents through advection. We propose a new\\nmulti-variate Observing System Simulation Experiment (OSSE) emulating 20 years\\nof SSH and SST satellite observations. We train an Attention-Based\\nEncoder-Decoder deep learning network (abed) on this data, comparing two\\nsettings: one with access to ground truth during training and one without. On\\nour OSSE, we compare abed reconstructions when trained using either supervised\\nor unsupervised loss functions, with or without SST information. We evaluate\\nthe SSH interpolations in terms of eddy detection. We also introduce a new way\\nto transfer the learning from simulation to observations by doing a supervised\\npre-training on our OSSE followed by an unsupervised fine-tuning on satellite\\ndata. On real SSH observations from the Ocean Data Challenge 2021, we find that\\nthis learning strategy combined with the use of SST leads to a decrease of 24%\\nof the root mean squared error compared to duacs.', 'tags': 'cs.LG', 'thumb_url': 'static/thumb/2310.07626.jpg', 'time': 'May 03 2024', 'title': 'Learning of Sea Surface Height Interpolation from Multi-variate\\n  Simulated Satellite Observations', 'utags': [], 'weight': 2.6445112101154193}, {'authors': 'Karl Van Wyk, Ankur Handa, Viktor Makoviychuk, Yijie Guo, Arthur Allshire, Nathan D. Ratliff', 'id': '2405.02250', 'summary': 'Robotics policies are always subjected to complex, second order dynamics that\\nentangle their actions with resulting states. In reinforcement learning (RL)\\ncontexts, policies have the burden of deciphering these complicated\\ninteractions over massive amounts of experience and complex reward functions to\\nlearn how to accomplish tasks. Moreover, policies typically issue actions\\ndirectly to controllers like Operational Space Control (OSC) or joint PD\\ncontrol, which induces straightline motion towards these action targets in task\\nor joint space. However, straightline motion in these spaces for the most part\\ndo not capture the rich, nonlinear behavior our robots need to exhibit,\\nshifting the burden of discovering these behaviors more completely to the\\nagent. Unlike these simpler controllers, geometric fabrics capture a much\\nricher and desirable set of behaviors via artificial, second order dynamics\\ngrounded in nonlinear geometry. These artificial dynamics shift the\\nuncontrolled dynamics of a robot via an appropriate control law to form\\nbehavioral dynamics. Behavioral dynamics unlock a new action space and safe,\\nguiding behavior over which RL policies are trained. Behavioral dynamics enable\\nbang-bang-like RL policy actions that are still safe for real robots, simplify\\nreward engineering, and help sequence real-world, high-performance policies. We\\ndescribe the framework more generally and create a specific instantiation for\\nthe problem of dexterous, in-hand reorientation of a cube by a highly actuated\\nrobot hand.', 'tags': 'cs.RO', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Geometric Fabrics: a Safe Guiding Medium for Policy Learning', 'utags': [], 'weight': 2.6476014878931973}, {'authors': 'Hugo Laurenon, Lo Tronchon, Matthieu Cord, Victor Sanh', 'id': '2405.02246', 'summary': 'The growing interest in vision-language models (VLMs) has been driven by\\nimprovements in large language models and vision transformers. Despite the\\nabundance of literature on this subject, we observe that critical decisions\\nregarding the design of VLMs are often not justified. We argue that these\\nunsupported decisions impede progress in the field by making it difficult to\\nidentify which choices improve model performance. To address this issue, we\\nconduct extensive experiments around pre-trained models, architecture choice,\\ndata, and training methods. Our consolidation of findings includes the\\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\\nIdefics2 achieves state-of-the-art performance within its size category across\\nvarious multimodal benchmarks, and is often on par with models four times its\\nsize. We release the model (base, instructed, and chat) along with the datasets\\ncreated for its training.', 'tags': 'cs.CV, cs.AI', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'What matters when building vision-language models?', 'utags': [], 'weight': 2.6529834323376416}, {'authors': 'Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, Anders Johannsen', 'id': '2403.00462', 'summary': 'Spurred by recent advances in Large Language Models (LLMs), virtual\\nassistants are poised to take a leap forward in terms of their dialogue\\ncapabilities. Yet a major bottleneck to achieving genuinely transformative\\ntask-oriented dialogue capabilities remains the scarcity of high quality data.\\nExisting datasets, while impressive in scale, have limited domain coverage and\\ncontain few genuinely challenging conversational phenomena; those which are\\npresent are typically unlabelled, making it difficult to assess the strengths\\nand weaknesses of models without time-consuming and costly human evaluation.\\nMoreover, creating high quality dialogue data has until now required\\nconsiderable human input, limiting both the scale of these datasets and the\\nability to rapidly bootstrap data for a new target domain. We aim to overcome\\nthese issues with LUCID, a modularised and highly automated LLM-driven data\\ngeneration system that produces realistic, diverse and challenging dialogues.\\nWe use LUCID to generate a seed dataset of 4,277 conversations across 100\\nintents to demonstrate its capabilities, with a human review finding\\nconsistently high quality labels in the generated data.', 'tags': 'cs.CL, I.2.7', 'thumb_url': 'static/thumb/2403.00462.jpg', 'time': 'May 03 2024', 'title': 'LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues', 'utags': [], 'weight': 2.6549857471524567}, {'authors': 'Hanwen Qi, Edward Sun, Harry Zhang', 'id': '2405.02243', 'summary': 'Behavioral cloning, or more broadly, learning from demonstrations (LfD) is a\\npriomising direction for robot policy learning in complex scenarios. Albeit\\nbeing straightforward to implement and data-efficient, behavioral cloning has\\nits own drawbacks, limiting its efficacy in real robot setups. In this work, we\\ntake one step towards improving learning from demonstration algorithms by\\nleveraging implicit energy-based policy models. Results suggest that in\\nselected complex robot policy learning scenarios, treating supervised policy\\nlearning with an implicit model generally performs better, on average, than\\ncommonly used neural network-based explicit models, especially in the cases of\\napproximating potentially discontinuous and multimodal functions.', 'tags': 'cs.RO', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Towards Improving Learning from Demonstration Algorithms via MCMC\\n  Methods', 'utags': [], 'weight': 2.657566765670975}, {'authors': 'Xuxin Cheng, Heng Yu, Harry Zhang, Wenxing Deng', 'id': '2405.02241', 'summary': 'We present a novel method for robotic manipulation tasks in human\\nenvironments that require reasoning about the 3D geometric relationship between\\na pair of objects. Traditional end-to-end trained policies, which map from\\npixel observations to low-level robot actions, struggle to reason about complex\\npose relationships and have difficulty generalizing to unseen object\\nconfigurations. To address these challenges, we propose a method that learns to\\nreason about the 3D geometric relationship between objects, focusing on the\\nrelationship between key parts on one object with respect to key parts on\\nanother object. Our standalone model utilizes Weighted SVD to reason about both\\npose relationships between articulated parts and between free-floating objects.\\nThis approach allows the robot to understand the relationship between the oven\\ndoor and the oven body, as well as the relationship between the lasagna plate\\nand the oven, for example. By considering the 3D geometric relationship between\\nobjects, our method enables robots to perform complex manipulation tasks that\\nreason about object-centric representations. We open source the code and\\ndemonstrate the results here', 'tags': 'cs.RO', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD', 'utags': [], 'weight': 2.6585274138191233}, {'authors': 'Elika Bozorgi, Saber Soleimani, Sakher Khalil Alqaiidi, Hamid Reza Arabnia, Krzysztof Kochut', 'id': '2405.02240', 'summary': 'Graph is an important data representation which occurs naturally in the real\\nworld applications \\\\cite{goyal2018graph}. Therefore, analyzing graphs provides\\nusers with better insights in different areas such as anomaly detection\\n\\\\cite{ma2021comprehensive}, decision making \\\\cite{fan2023graph}, clustering\\n\\\\cite{tsitsulin2023graph}, classification \\\\cite{wang2021mixup} and etc.\\nHowever, most of these methods require high levels of computational time and\\nspace. We can use other ways like embedding to reduce these costs. Knowledge\\ngraph (KG) embedding is a technique that aims to achieve the vector\\nrepresentation of a KG. It represents entities and relations of a KG in a\\nlow-dimensional space while maintaining the semantic meanings of them. There\\nare different methods for embedding graphs including random walk-based methods\\nsuch as node2vec, metapath2vec and regpattern2vec. However, most of these\\nmethods bias the walks based on a rigid pattern usually hard-coded in the\\nalgorithm. In this work, we introduce \\\\textit{subgraph2vec} for embedding KGs\\nwhere walks are run inside a user-defined subgraph. We use this embedding for\\nlink prediction and prove our method has better performance in most cases in\\ncomparison with the previous ones.', 'tags': 'cs.LG', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Subgraph2vec: A random walk-based algorithm for embedding knowledge\\n  graphs', 'utags': [], 'weight': 2.6590250990043085}, {'authors': 'Yongxin Zhou, Fabien Ringeval, Franois Portet', 'id': '2307.12371', 'summary': 'Automatic dialogue summarization is a well-established task with the goal of\\ndistilling the most crucial information from human conversations into concise\\ntextual summaries. However, most existing research has predominantly focused on\\nsummarizing factual information, neglecting the affective content, which can\\nhold valuable insights for analyzing, monitoring, or facilitating human\\ninteractions. In this paper, we introduce and assess a set of measures\\nPSentScore, aimed at quantifying the preservation of affective content in\\ndialogue summaries. Our findings indicate that state-of-the-art summarization\\nmodels do not preserve well the affective content within their summaries.\\nMoreover, we demonstrate that a careful selection of the training set for\\ndialogue samples can lead to improved preservation of affective content in the\\ngenerated summaries, albeit with a minor reduction in content-related metrics.', 'tags': 'cs.CL', 'thumb_url': 'static/thumb/2307.12371.jpg', 'time': 'May 03 2024', 'title': 'PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization', 'utags': [], 'weight': 2.660738061967271}, {'authors': 'Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil', 'id': '2312.06733', 'summary': 'LiDAR Upsampling is a challenging task for the perception systems of robots\\nand autonomous vehicles, due to the sparse and irregular structure of\\nlarge-scale scene contexts. Recent works propose to solve this problem by\\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\\nproblem in 2D image space. Although their methods can generate high-resolution\\nrange images with fine-grained details, the resulting 3D point clouds often\\nblur out details and predict invalid points. In this paper, we propose TULIP, a\\nnew method to reconstruct high-resolution LiDAR point clouds from\\nlow-resolution LiDAR input. We also follow a range image-based approach but\\nspecifically modify the patch and window geometries of a Swin-Transformer-based\\nnetwork to better fit the characteristics of range images. We conducted several\\nexperiments on three public real-world and simulated datasets. TULIP\\noutperforms state-of-the-art methods in all relevant metrics and generates\\nrobust and more realistic point clouds than prior works.', 'tags': 'cs.CV', 'thumb_url': 'static/thumb/2312.06733.jpg', 'time': 'May 03 2024', 'title': 'TULIP: Transformer for Upsampling of LiDAR Point Clouds', 'utags': [], 'weight': 2.6620459323376418}, {'authors': 'Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley', 'id': '2310.09653', 'summary': 'We propose SelfVC, a training strategy to iteratively improve a voice\\nconversion model with self-synthesized examples. Previous efforts on voice\\nconversion focus on factorizing speech into explicitly disentangled\\nrepresentations that separately encode speaker characteristics and linguistic\\ncontent. However, disentangling speech representations to capture such\\nattributes using task-specific loss terms can lead to information loss. In this\\nwork, instead of explicitly disentangling attributes with loss terms, we\\npresent a framework to train a controllable voice conversion model on entangled\\nspeech representations derived from self-supervised learning (SSL) and speaker\\nverification models. First, we develop techniques to derive prosodic\\ninformation from the audio signal and SSL representations to train predictive\\nsubmodules in the synthesis model. Next, we propose a training strategy to\\niteratively improve the synthesis model for voice conversion, by creating a\\nchallenging training objective using self-synthesized examples. We demonstrate\\nthat incorporating such self-synthesized examples during training improves the\\nspeaker similarity of generated speech as compared to a baseline voice\\nconversion model trained solely on heuristically perturbed inputs. Our\\nframework is trained without any text and achieves state-of-the-art results in\\nzero-shot voice conversion on metrics evaluating naturalness, speaker\\nsimilarity, and intelligibility of synthesized audio.', 'tags': 'cs.SD, cs.AI, eess.AS', 'thumb_url': 'static/thumb/2310.09653.jpg', 'time': 'May 03 2024', 'title': 'SelfVC: Voice Conversion With Iterative Refinement using Self\\n  Transformations', 'utags': [], 'weight': 2.6629487101154194}, {'authors': 'Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, Matteo Papini', 'id': '2405.02235', 'summary': 'Policy gradient (PG) methods are successful approaches to deal with\\ncontinuous reinforcement learning (RL) problems. They learn stochastic\\nparametric (hyper)policies by either exploring in the space of actions or in\\nthe space of parameters. Stochastic controllers, however, are often undesirable\\nfrom a practical perspective because of their lack of robustness, safety, and\\ntraceability. In common practice, stochastic (hyper)policies are learned only\\nto deploy their deterministic version. In this paper, we make a step towards\\nthe theoretical understanding of this practice. After introducing a novel\\nframework for modeling this scenario, we study the global convergence to the\\nbest deterministic policy, under (weak) gradient domination assumptions. Then,\\nwe illustrate how to tune the exploration level used for learning to optimize\\nthe trade-off between the sample complexity and the performance of the deployed\\ndeterministic policy. Finally, we quantitatively compare action-based and\\nparameter-based exploration, giving a formal guise to intuitive results.', 'tags': 'cs.LG', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Learning Optimal Deterministic Policies with Stochastic Policy Gradients', 'utags': [], 'weight': 2.6632264878931973}, {'authors': \"Qiqi Su, Christos Kloukinas, Artur d'Avila Garcez\", 'id': '2311.16834', 'summary': 'Multivariate time series have many applications, from healthcare and\\nmeteorology to life science. Although deep learning models have shown excellent\\npredictive performance for time series, they have been criticised for being\\n\"black-boxes\" or non-interpretable. This paper proposes a novel modular neural\\nnetwork model for multivariate time series prediction that is interpretable by\\nconstruction. A recurrent neural network learns the temporal dependencies in\\nthe data while an attention-based feature selection component selects the most\\nrelevant features and suppresses redundant features used in the learning of the\\ntemporal dependencies. A modular deep network is trained from the selected\\nfeatures independently to show the users how features influence outcomes,\\nmaking the model interpretable. Experimental results show that this approach\\ncan outperform state-of-the-art interpretable Neural Additive Models (NAM) and\\nvariations thereof in both regression and classification of time series tasks,\\nachieving a predictive performance that is comparable to the top\\nnon-interpretable methods for time series, LSTM and XGBoost.', 'tags': 'cs.LG, cs.AI', 'thumb_url': 'static/thumb/2311.16834.jpg', 'time': 'May 03 2024', 'title': 'FocusLearn: Fully-Interpretable, High-Performance Modular Neural\\n  Networks for Time Series', 'utags': [], 'weight': 2.6637357471524568}, {'authors': 'Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur', 'id': '2405.02228', 'summary': 'Automatic citation generation for sentences in a document or report is\\nparamount for intelligence analysts, cybersecurity, news agencies, and\\neducation personnel. In this research, we investigate whether large language\\nmodels (LLMs) are capable of generating references based on two forms of\\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\\nthe title of a mentioned article when given a sentence from a different\\narticle. To demonstrate where LLM stands in this task, we introduce a large\\ndataset called REASONS comprising abstracts of the 12 most popular domains of\\nscientific research on arXiv. From around 20K research articles, we make the\\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\\npercentage (PP) to minimize the hallucination rate (HR). When tested with\\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\\nHR across all domains and models decreased by an average of 41.93% and the PP\\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\\nstruggle to understand context, but the extent of this issue was small in\\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\\nreliability of RAG for automated citation generation tasks.', 'tags': 'cs.CL, cs.AI, cs.IR', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\\n  Sentences using Public and Proprietary LLMs', 'utags': [], 'weight': 2.6676709323376415}, {'authors': 'Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen', 'id': '2405.01461', 'summary': 'Is the Text to Motion model robust? Recent advancements in Text to Motion\\nmodels primarily stem from more accurate predictions of specific actions.\\nHowever, the text modality typically relies solely on pre-trained Contrastive\\nLanguage-Image Pretraining (CLIP) models. Our research has uncovered a\\nsignificant issue with the text-to-motion model: its predictions often exhibit\\ninconsistent outputs, resulting in vastly different or even incorrect poses\\nwhen presented with semantically similar or identical text inputs. In this\\npaper, we undertake an analysis to elucidate the underlying causes of this\\ninstability, establishing a clear link between the unpredictability of model\\noutputs and the erratic attention patterns of the text encoder module.\\nConsequently, we introduce a formal framework aimed at addressing this issue,\\nwhich we term the Stable Text-to-Motion Framework (SATO). SATO consists of\\nthree modules, each dedicated to stable attention, stable prediction, and\\nmaintaining a balance between accuracy and robustness trade-off. We present a\\nmethodology for constructing an SATO that satisfies the stability of attention\\nand prediction. To verify the stability of the model, we introduced a new\\ntextual synonym perturbation dataset based on HumanML3D and KIT-ML. Results\\nshow that SATO is significantly more stable against synonyms and other slight\\nperturbations while keeping its high accuracy performance.', 'tags': 'cs.CV', 'thumb_url': 'static/thumb/2405.01461.jpg', 'time': 'May 03 2024', 'title': 'SATO: Stable Text-to-Motion Framework', 'utags': [], 'weight': 2.6698352841894937}, {'authors': 'Lujing Zhang, Aaron Roth, Linjun Zhang', 'id': '2405.02225', 'summary': 'This paper introduces a framework for post-processing machine learning models\\nso that their predictions satisfy multi-group fairness guarantees. Based on the\\ncelebrated notion of multicalibration, we introduce $(\\\\mathbf{s},\\\\mathcal{G},\\n\\\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\\nmulti-dimensional mappings $\\\\mathbf{s}$, constraint set $\\\\mathcal{G}$, and a\\npre-specified threshold level $\\\\alpha$. We propose associated algorithms to\\nachieve this notion in general settings. This framework is then applied to\\ndiverse scenarios encompassing different fairness concerns, including false\\nnegative rate control in image segmentation, prediction set conditional\\nuncertainty quantification in hierarchical classification, and de-biased text\\ngeneration in language models. We conduct numerical studies on several datasets\\nand tasks.', 'tags': 'stat.ML, cs.AI, cs.CY, cs.LG, stat.ME', 'thumb_url': '', 'time': 'May 03 2024', 'title': 'Fair Risk Control: A Generalized Framework for Calibrating Multi-group\\n  Fairness Risks', 'utags': [], 'weight': 2.6723237101154194}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Your string\n",
    "string_data = aa.find_all('script')[1].text.split('\\n')[1][13:-1]\n",
    "\n",
    "# Convert the string to list of dictionaries\n",
    "data_list = json.loads(string_data)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\n",
      "  language models 2405.02287 cs.CL, cs.AI, cs.CV\n",
      "1 DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\n",
      "  Videos 2405.02280 cs.CV\n",
      "2 A Careful Examination of Large Language Model Performance on Grade\n",
      "  School Arithmetic 2405.00332 cs.CL, cs.AI, cs.LG\n",
      "3 MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\n",
      "  Models 2309.12284 cs.CL, cs.AI\n",
      "4 InceptionXML: A Lightweight Framework with Synchronized Negative\n",
      "  Sampling for Short Text Extreme Classification 2109.07319 cs.CL, cs.AI, cs.LG\n",
      "5 Structural Pruning of Pre-trained Language Models via Neural\n",
      "  Architecture Search 2405.02267 cs.LG, cs.CL\n",
      "6 On the test-time zero-shot generalization of vision-language models: Do\n",
      "  we really need prompt learning? 2405.02266 cs.CV\n",
      "7 Constrained Reinforcement Learning Under Model Mismatch 2405.01327 cs.LG\n",
      "8 Hysteresis Compensation of Flexible Continuum Manipulator using RGBD\n",
      "  Sensing and Temporal Convolutional Network 2402.11319 cs.RO, cs.AI\n",
      "9 BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex\n",
      "  Selectivity 2310.04420 cs.LG, q-bio.NC\n",
      "10 Learning of Sea Surface Height Interpolation from Multi-variate\n",
      "  Simulated Satellite Observations 2310.07626 cs.LG\n",
      "11 Geometric Fabrics: a Safe Guiding Medium for Policy Learning 2405.02250 cs.RO\n",
      "12 What matters when building vision-language models? 2405.02246 cs.CV, cs.AI\n",
      "13 LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues 2403.00462 cs.CL, I.2.7\n",
      "14 Towards Improving Learning from Demonstration Algorithms via MCMC\n",
      "  Methods 2405.02243 cs.RO\n",
      "15 WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD 2405.02241 cs.RO\n",
      "16 Subgraph2vec: A random walk-based algorithm for embedding knowledge\n",
      "  graphs 2405.02240 cs.LG\n",
      "17 PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization 2307.12371 cs.CL\n",
      "18 TULIP: Transformer for Upsampling of LiDAR Point Clouds 2312.06733 cs.CV\n",
      "19 SelfVC: Voice Conversion With Iterative Refinement using Self\n",
      "  Transformations 2310.09653 cs.SD, cs.AI, eess.AS\n",
      "20 Learning Optimal Deterministic Policies with Stochastic Policy Gradients 2405.02235 cs.LG\n",
      "21 FocusLearn: Fully-Interpretable, High-Performance Modular Neural\n",
      "  Networks for Time Series 2311.16834 cs.LG, cs.AI\n",
      "22 REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\n",
      "  Sentences using Public and Proprietary LLMs 2405.02228 cs.CL, cs.AI, cs.IR\n",
      "23 SATO: Stable Text-to-Motion Framework 2405.01461 cs.CV\n",
      "24 Fair Risk Control: A Generalized Framework for Calibrating Multi-group\n",
      "  Fairness Risks 2405.02225 stat.ML, cs.AI, cs.CY, cs.LG, stat.ME\n"
     ]
    }
   ],
   "source": [
    "for idp, paper in enumerate(data_list):\n",
    "    # print(idp, paper)\n",
    "    print(idp, paper['title'], paper['id'], paper['tags'])\n",
    "    # for k, v in paper.items():\n",
    "    #     print(k, v)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': \"Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Mikel Artetxe, Yi Tay\",\n",
       "  'id': '2405.02287',\n",
       "  'summary': \"We introduce Vibe-Eval: a new open benchmark and framework for evaluating\\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\\nincluding 100 of hard difficulty, complete with gold-standard responses\\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\\n(ii) rigorously testing and probing the capabilities of present frontier\\nmodels. Notably, our hard set contains >50% questions that all frontier models\\nanswer incorrectly. We explore the nuances of designing, evaluating, and\\nranking models on ultra challenging prompts. We also discuss trade-offs between\\nhuman and automatic evaluation, and show that automatic model evaluation using\\nReka Core roughly correlates to human judgment. We offer free API access for\\nthe purpose of lightweight evaluation and plan to conduct formal human\\nevaluations for public models that perform well on the Vibe-Eval's automatic\\nscores. We release the evaluation code and data, see\\nhttps://github.com/reka-ai/reka-vibe-eval\",\n",
       "  'tags': 'cs.CL, cs.AI, cs.CV',\n",
       "  'thumb_url': '',\n",
       "  'time': 'May 03 2024',\n",
       "  'title': 'Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\\n  language models',\n",
       "  'utags': [],\n",
       "  'weight': 2.6113746360413455},\n",
       " {'authors': 'Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki',\n",
       "  'id': '2405.02280',\n",
       "  'summary': 'Existing VLMs can track in-the-wild 2D video objects while current generative\\nmodels provide powerful visual priors for synthesizing novel views for the\\nhighly under-constrained 2D-to-3D object lifting. Building upon this exciting\\nprogress, we present DreamScene4D, the first approach that can generate\\nthree-dimensional dynamic scenes of multiple objects from monocular in-the-wild\\nvideos with large object motion across occlusions and novel viewpoints. Our key\\ninsight is to design a \"decompose-then-recompose\" scheme to factorize both the\\nwhole video scene and each object\\'s 3D motion. We first decompose the video\\nscene by using open-vocabulary mask trackers and an adapted image diffusion\\nmodel to segment, track, and amodally complete the objects and background in\\nthe video. Each object track is mapped to a set of 3D Gaussians that deform and\\nmove in space and time. We also factorize the observed motion into multiple\\ncomponents to handle fast motion. The camera motion can be inferred by\\nre-rendering the background to match the video frames. For the object motion,\\nwe first model the object-centric deformation of the objects by leveraging\\nrendering losses and multi-view generative priors in an object-centric frame,\\nthen optimize object-centric to world-frame transformations by comparing the\\nrendered outputs against the perceived pixel and optical flow. Finally, we\\nrecompose the background and objects and optimize for relative object scales\\nusing monocular depth prediction guidance. We show extensive results on the\\nchallenging DAVIS, Kubric, and self-captured videos, detail some limitations,\\nand provide future directions. Besides 4D scene generation, our results show\\nthat DreamScene4D enables accurate 2D point motion tracking by projecting the\\ninferred 3D trajectories to 2D, while never explicitly trained to do so.',\n",
       "  'tags': 'cs.CV',\n",
       "  'thumb_url': '',\n",
       "  'time': 'May 03 2024',\n",
       "  'title': 'DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\\n  Videos',\n",
       "  'utags': [],\n",
       "  'weight': 2.6143954693746787}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
